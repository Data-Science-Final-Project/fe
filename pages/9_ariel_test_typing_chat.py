import os, sys, json, uuid, asyncio, re
from datetime import datetime

import streamlit as st
import torch, fitz, docx, numpy as np
from openai import AsyncOpenAI, OpenAI
from dotenv import load_dotenv
from streamlit_js import st_js, st_js_blocking

from app_resources import mongo_client, pinecone_client, model

#  ENV & GLOBALS 
load_dotenv()
DATABASE_NAME  = os.getenv("DATABASE_NAME")
OPENAI_API_KEY = os.getenv("OPEN_AI")

client_async_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)
client_sync_openai  = OpenAI    (api_key=OPENAI_API_KEY)

torch.classes.__path__ = []      # Streamlit-Torch bug-workaround
os.environ["TOKENIZERS_PARALLELISM"] = "false"

judgment_index = pinecone_client.Index("judgments-names")
law_index      = pinecone_client.Index("laws-names")

db = mongo_client[DATABASE_NAME]
judgment_collection = db["judgments"]
law_collection      = db["laws"]
conversation_coll   = db["conversations"]

#  UI CSS 
st.set_page_config(page_title="Ask Mini Lawyer", page_icon="锔", layout="wide")
st.markdown("""
<style>
.chat-container{background:#1E1E1E;padding:20px;border-radius:10px}
.chat-header{color:#4CAF50;font-size:36px;font-weight:bold;text-align:center}
.user-message{background:#4CAF50;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.bot-message{background:#44475a;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.timestamp{font-size:0.75em;color:#bbb}
.law-card{border:1px solid #e0e0e0;border-radius:10px;padding:20px;margin-bottom:15px;
          box-shadow:0 2px 4px rgba(0,0,0,0.1);background:#f9f9f9}
.law-title{font-size:20px;font-weight:bold;color:#333}
.law-description{font-size:16px;color:#444;margin:10px 0}
.law-meta{font-size:14px;color:#555}
.stButton>button{background:#7ce38b;color:#fff;font-size:14px;border:none;padding:8px 16px;
                 border-radius:5px;cursor:pointer}
.stButton>button:hover{background:#69d67a}
</style>
""", unsafe_allow_html=True)

app_mode = st.sidebar.selectbox("Choose module", ["Chat Assistant", "Legal Finder"])

#  GENERAL HELPERS 
ls_get = lambda k: st_js_blocking(f"return localStorage.getItem('{k}');", key="ls_"+k)
ls_set = lambda k,v: st_js(f"localStorage.setItem('{k}', '{v}');")

read_pdf  = lambda f: "".join(p.get_text() for p in fitz.open(stream=f.read(), filetype="pdf"))
read_docx = lambda f: "\n".join(p.text for p in docx.Document(f).paragraphs)

def add_msg(role, txt):
    st.session_state.setdefault("messages", []).append(
        {"role": role, "content": txt, "timestamp": datetime.now().strftime("%H:%M:%S")}
    )

def show_msgs():
    for m in st.session_state.get("messages", []):
        css = "user-message" if m["role"] == "user" else "bot-message"
        st.markdown(f"<div class='{css}'>{m['content']}<div class='timestamp'>{m['timestamp']}</div></div>",
                    unsafe_allow_html=True)

def chunk_text(txt, L=450):
    sent = re.split(r'(?:\.|\?|!)\s+', txt)
    out, cur = [], ""
    for s in sent:
        if len(cur)+len(s) > L and cur:
            out.append(cur.strip()); cur = s
        else:
            cur += " " + s
    if cur.strip(): out.append(cur.strip())
    return out[:20]

#  Hebrew-only enforcement 
contains_english = lambda t: bool(re.search(r"[A-Za-z]", t))
def ensure_hebrew(t):
    if not contains_english(t): return t
    prompt = "转专 转 拽住  注专转    转:\n" + t
    r = client_sync_openai.chat.completions.create(
        model="gpt-3.5-turbo", messages=[{"role":"user","content":prompt}],
        temperature=0, max_tokens=len(t)//2)
    return r.choices[0].message.content.strip()

#  CLASSIFICATION 
CATEGORIES = {
 "转_驻专": {"regex":[r"驻专(|)|termination notice"]},
 "_注":   {"regex":[r"住\s+注|employment agreement"]},
 "NDA":          {"regex":[r"住转|confidentiality"]},
 "CEASE_DESIST": {"regex":[r"|驻住拽|cease and desist"]},
 "转拽":        {"regex":[r"转拽|by.?law|policy"]},
 "转_转注":    {"regex":[r"转\s+转注|转注|转注"]},
 "驻住拽_":      {"regex":[r"驻住拽[-\s]?|转.?砖驻"]},
 "转_专":     {"regex":[]}
}
CLS_SYSTEM = "转 住 住 砖驻. 专 转转 转 : " + ", ".join(CATEGORIES.keys())
def classify_doc(txt:str)->str:
    sample = txt[:800] + "\n---\n" + txt[-800:]
    try:
        resp = client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"system","content":CLS_SYSTEM+"\n"+sample}],
            temperature=0, max_tokens=5)
        label = resp.choices[0].message.content.strip()
    except Exception:
        label = "转_专"
    for lab,d in CATEGORIES.items():
        if any(re.search(p,txt,re.I) for p in d["regex"]): label = lab
    return label if label in CATEGORIES else "转_专"

#  TEMPLATES 
H = lambda s: s + "  **注 注专转    转.**"
PROMPTS = {
 "转_驻专":dict(summary=H("住 转 驻专: 1. 驻专 注 转专, 2. 转 转砖, 3. 爪注 爪."),
                      answer =H("转 注\" -注. 砖 专拽 注 住 转 拽 注 专; 爪 拽专  注.")),
 "_注":  dict(summary=H("住  注: 1. 转 注住拽, 2. 住注驻 住转 -转专转, 3. 住 爪转."),
                      answer =H("转 注\" -注. 转 转 住注驻  爪 拽/驻住状 转.")),
 "NDA":          dict(summary=H("住 NDA: 1. 专转 注 住, 2. 转拽驻转 住, 3. 爪注 驻."),
                      answer =H("转 注\" 拽-专. 驻专 砖转 砖驻转 砖 驻专转 -NDA.")),
 "CEASE_DESIST": dict(summary=H("住 转 专: 1. 注转, 2. 专砖转, 3. 转  驻."),
                      answer =H("转 注\" 爪. 注专 拽转 住 砖  转.")),
 "转拽":        dict(summary=H("住 转拽/转: 1. 专转, 2. 转/转, 3. 住 -爪转."),
                      answer =H("转 注\" 专转. 住专 转拽祝 住注驻 转拽.")),
 "转_转注":    dict(summary=H("住 转 转注: 1. 注转, 2. 住注, 3.   ."),
                      answer =H("转 注\". 转 转 转注 转 转拽转 住专  专.")),
 "驻住拽_":      dict(summary=H("住 驻住拽-: 1. 砖 砖驻转, 2. 拽注转, 3. ."),
                      answer =H("转 注\". 住专 转 转 转-砖驻 转拽驻.")),
 "_":            dict(summary=H("住 转 住: 转拽爪专, 拽转 注拽专转, 砖转."),
                      answer =H("转 注\". 砖 注 住 住 拽 专."))
}
tmpl = lambda lbl, kind: PROMPTS.get(lbl, PROMPTS["_"])[kind]

#  RETRIEVAL (RAG) 
def embed(t): return model.encode([t], normalize_embeddings=True)[0]

async def retrieve(query, doc):
    q_emb = embed(query)
    secs  = [embed(c) for c in chunk_text(doc)] if doc else []
    cand  = {"law":{}, "judg":{}}

    async def add(match, kind):
        meta, score = match.get("metadata",{}), match.get("score",0)
        key   = "IsraelLawID" if kind=="law" else "CaseNumber"
        _id   = meta.get(key); coll = law_collection if kind=="law" else judgment_collection
        if not _id: return
        d = coll.find_one({key:_id}); 
        if not d: return
        cand[kind].setdefault(_id, {"doc":d,"scores":[]})["scores"].append(score)

    # scan each section
    async def scan(e):
        rl, rj = await asyncio.gather(
            asyncio.to_thread(law_index.query,      vector=e.tolist(), top_k=1, include_metadata=True),
            asyncio.to_thread(judgment_index.query, vector=e.tolist(), top_k=1, include_metadata=True))
        [await add(m,"law")  for m in rl.get("matches",[])]
        [await add(m,"judg") for m in rj.get("matches",[])]

    await asyncio.gather(*(scan(e) for e in secs))
    # add global matches
    for m in law_index.query(vector=q_emb.tolist(), top_k=3, include_metadata=True).get("matches",[]):   await add(m,"law")
    for m in judgment_index.query(vector=q_emb.tolist(), top_k=3, include_metadata=True).get("matches",[]): await add(m,"judg")

    top = lambda d: sorted(d.values(), key=lambda x:-np.mean(x["scores"]))[:3]
    return [x["doc"] for x in top(cand["law"])], [x["doc"] for x in top(cand["judg"])]

#  SELF-CHECK  (citations + he) 
async def citations_ok(ans:str)->bool:
    if contains_english(ans): return False
    probe = "Does every claim have an explicit citation? Answer Yes/No."
    try:
        r = await client_async_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"user","content": probe + "\n" + ans}],
            temperature=0, max_tokens=3)
        return "yes" in r.choices[0].message.content.lower()
    except: return True   # fail-open

#  CHAT ASSISTANT 
def chat_assistant():
    st.markdown('<div class="chat-header"> Ask Mini Lawyer</div>', unsafe_allow_html=True)

    # session state bootstrap
    if "cid" not in st.session_state:
        cid = ls_get("AMLChatId") or str(uuid.uuid4())
        ls_set("AMLChatId", cid)
        st.session_state.cid = cid
    if "messages" not in st.session_state:
        conv = conversation_coll.find_one({"local_storage_id": st.session_state.cid})
        st.session_state["messages"] = conv.get("messages", []) if conv else []
    st.session_state.setdefault("name", None)

    # first-run name prompt
    if not st.session_state.get("name"):
        with st.form("name_form"):
            n = st.text_input("住 砖 转转 砖:", key="user_name_field")
            submitted = st.form_submit_button("转")
        if submitted and n:
            st.session_state["name"] = n
            add_msg("assistant", f"砖 {n},  驻砖专 注专?")
            conversation_coll.update_one(
                {"local_storage_id": st.session_state.cid},
                {"$set": {"user_name": n,
                          "messages": st.session_state["messages"]}},
                upsert=True)
            st.rerun()
        return   

    # show chat
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    show_msgs()
    st.markdown("</div>", unsafe_allow_html=True)

    # file upload
    up = st.file_uploader(" 注 住", type=["pdf","docx"])
    if up:
        raw = read_pdf(up) if up.type=="application/pdf" else read_docx(up)
        st.session_state.doc     = "\n".join(l for l in raw.splitlines() if re.search(r"[-转]", l))
        st.session_state.doctype = classify_doc(st.session_state.doc)
        st.success(f"住 住: {st.session_state.doctype}")

    # summary button
    if hasattr(st.session_state, "doc") and st.button(" 住"):
        with st.spinner("住..."):
            prompt = tmpl(st.session_state.doctype,"summary") + "\n" + st.session_state.doc
            r = asyncio.run(client_async_openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}],
                temperature=0.1, max_tokens=700))
            st.session_state.summary = ensure_hebrew(r.choices[0].message.content.strip())
    if st.session_state.get("summary"):
        st.markdown("### 住:"); st.info(st.session_state.summary)

    # async handler
    async def handle(q):
        laws, judg = await retrieve(q, st.session_state.get("doc",""))
        doc_sn = st.session_state.get("doc","")[:1500]
        law_txt = "\n\n".join(d.get("Description","")[:800] for d in laws)
        jud_txt = "\n\n".join(d.get("Description","")[:800] for d in judg)

        sys = tmpl(st.session_state.get("doctype","_"),"answer") + \
              "\n\n--- 住 ---\n" + doc_sn + \
              "\n\n--- 拽 ---\n" + law_txt + \
              "\n\n--- 驻住拽  ---\n" + jud_txt
        r = await client_async_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"system","content":sys},
                      {"role":"user","content":q}],
            temperature=0, max_tokens=700)
        ans = ensure_hebrew(r.choices[0].message.content.strip())
        if not await citations_ok(ans):
            ans = "锔 转砖  注 拽专专 (注专转  + 住转转). 住 砖 爪专 拽转 转专."
        return ans

    # ask form
    with st.form("ask", clear_on_submit=True):
        q = st.text_area("拽 砖 砖驻转:", height=100)
        send = st.form_submit_button("砖")
    if send and q.strip():
        ans = asyncio.run(handle(q.strip()))
        add_msg("user", q.strip()); add_msg("assistant", ans)
        conversation_coll.update_one(
            {"local_storage_id": st.session_state.cid},
            {"$set":{"messages": st.session_state["messages"],
                     "user_name": st.session_state["name"]}},
            upsert=True)
        st.rerun()

    # clear chat
    if st.button(" 拽"):
        conversation_coll.delete_one({"local_storage_id": st.session_state.cid})
        st_js("localStorage.clear();")
        st.session_state.clear()
        st.rerun()

#  LEGAL FINDER 
def legal_finder():
    st.title("Legal Finder")
    kind = st.selectbox("Search for", ["Judgment","Law"])
    scen = st.text_area("Scenario")
    if st.button("Find") and scen:
        emb  = embed(scen)
        idx  = judgment_index if kind=="Judgment" else law_index
        key  = "CaseNumber" if kind=="Judgment" else "IsraelLawID"
        res  = idx.query(vector=emb.tolist(), top_k=5, include_metadata=True)
        for m in res.get("matches", []):
            _id = m.get("metadata", {}).get(key); 
            coll= judgment_collection if kind=="Judgment" else law_collection
            doc = coll.find_one({key:_id}); 
            if not doc: continue
            st.markdown(
                f"<div class='law-card'><div class='law-title'>{doc.get('Name','')} (ID:{_id})</div>"
                f"<div class='law-description'>{doc.get('Description','')[:600]}...</div></div>",
                unsafe_allow_html=True)

#  MAIN 
if app_mode == "Chat Assistant":
    chat_assistant()
else:
    legal_finder()
