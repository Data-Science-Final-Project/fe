# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, json, uuid, asyncio, re
from datetime import datetime

import streamlit as st
import torch, fitz, docx, numpy as np
from bidi.algorithm import get_display       
from openai import AsyncOpenAI, OpenAI
from dotenv import load_dotenv
from streamlit_js import st_js, st_js_blocking

from app_resources import mongo_client, pinecone_client, model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENV & GLOBALS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
DATABASE_NAME  = os.getenv("DATABASE_NAME")
OPENAI_API_KEY = os.getenv("OPEN_AI")

client_async_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)
client_sync_openai  = OpenAI(api_key=OPENAI_API_KEY)

torch.classes.__path__ = []        # Streamlit-Torch bug-workaround
os.environ["TOKENIZERS_PARALLELISM"] = "false"

judgment_index = pinecone_client.Index("judgments-names")
law_index      = pinecone_client.Index("laws-names")

db = mongo_client[DATABASE_NAME]
judgment_collection = db["judgments"]
law_collection      = db["laws"]
conversation_coll   = db["conversations"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Ask Mini Lawyer", page_icon="âš–ï¸", layout="wide")
st.markdown("""
<style>
.chat-container{background:#1E1E1E;padding:20px;border-radius:10px}
.chat-header{color:#4CAF50;font-size:36px;font-weight:bold;text-align:center}
.user-message{background:#4CAF50;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.bot-message{background:#44475a;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.timestamp{font-size:0.75em;color:#bbb}
.law-card{border:1px solid #e0e0e0;border-radius:10px;padding:20px;margin-bottom:15px;
          box-shadow:0 2px 4px rgba(0,0,0,0.1);background:#f9f9f9}
.law-title{font-size:20px;font-weight:bold;color:#333}
.law-description{font-size:16px;color:#444;margin:10px 0}
.law-meta{font-size:14px;color:#555}
.stButton>button{background:#7ce38b;color:#fff;font-size:14px;border:none;padding:8px 16px;border-radius:5px;cursor:pointer}
.stButton>button:hover{background:#69d67a}
</style>
""", unsafe_allow_html=True)

app_mode = st.sidebar.selectbox("Choose module", ["Chat Assistant", "Legal Finder"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GENERAL HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ls_get = lambda k: st_js_blocking(f"return localStorage.getItem('{k}');", key="ls_"+k)
ls_set = lambda k,v: st_js(f"localStorage.setItem('{k}', '{v}');")

# ---------- RTL normalization ----------
heb  = re.compile(r'[×-×ª]')
SALUT= r'\b(×œ×›×‘×•×“|××¨|××¨\.?|×’×‘\'?|×’×‘×¨×ª|×“"×¨|×“"×¨\.?)\b'
def rtl_norm(t:str)->str:
    if not heb.search(t): return t
    try: t = get_display(t)
    except Exception: t = " ".join(w[::-1] if heb.search(w) else w for w in t.split())
    t = re.sub(SALUT, '', t)
    return re.sub(r'\s{2,}', ' ', t).strip()

read_pdf  = lambda f: "".join(rtl_norm(p.get_text()) for p in fitz.open(stream=f.read(), filetype="pdf"))
read_docx = lambda f: "\n".join(rtl_norm(p.text)     for p in docx.Document(f).paragraphs if p.text.strip())

def add_msg(role, txt):
    st.session_state.setdefault("messages", []).append(
        {"role": role, "content": txt, "timestamp": datetime.now().strftime("%H:%M:%S")})

def show_msgs():
    for m in st.session_state.get("messages", []):
        css = "user-message" if m["role"]=="user" else "bot-message"
        st.markdown(f"<div class='{css}'>{m['content']}<div class='timestamp'>{m['timestamp']}</div></div>", unsafe_allow_html=True)

def chunk_text(txt, L=450):
    sent=re.split(r'(?:\.|\?|!)\s+', txt); out,cur=[], ""
    for s in sent:
        if len(cur)+len(s)>L and cur: out.append(cur.strip()); cur=s
        else: cur+=" "+s
    if cur.strip(): out.append(cur.strip())
    return out[:20]

contains_english=lambda t: bool(re.search(r'[A-Za-z]', t))
def ensure_hebrew(t):
    if sum(contains_english(w) for w in t.split())<=3: return t
    r=client_sync_openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role":"user","content":"×ª×¨×’× ××ª ×”×˜×§×¡×˜ ×”×‘× ×œ×¢×‘×¨×™×ª ××œ××” ×•×œ×œ× ××™×œ×™× ×‘×× ×’×œ×™×ª:\n"+t}],
        temperature=0,max_tokens=len(t)//2)
    return r.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLASSIFICATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORIES = {
 "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ":{"regex":[r"×¤×™×˜×•×¨(?:×™×Ÿ|×™×)", r"×¡×™×•×\s+×”?×¢×¡×§×”", r"termination\s+notice"]},
 "×—×•×–×”_×¢×‘×•×“×”":  {"regex":[r"×”×¡×›×\s+×¢×‘×•×“×”|employment agreement"]},
 "NDA":          {"regex":[r"×¡×•×“×™×•×ª|confidentiality"]},
 "CEASE_DESIST": {"regex":[r"×—×“×œ|×œ×”×¤×¡×™×§|cease and desist"]},
 "×ª×§× ×•×Ÿ":        {"regex":[r"×ª×§× ×•×Ÿ|by.?law|policy"]},
 "×›×ª×‘_×ª×‘×™×¢×”":    {"regex":[r"×›×ª×‘\s+×ª×‘×™×¢×”|×”×ª×•×‘×¢|×”× ×ª×‘×¢"]},
 "×¤×¡×§_×“×™×Ÿ":      {"regex":[r"×¤×¡×§[-\s]?×“×™×Ÿ|×‘×™×ª.?××©×¤×˜"]},
 "××›×ª×‘_××—×¨":     {"regex":[]}
}
CLS_SYSTEM = "××ª×” ××¡×•×•×’ ××¡××›×™× ××©×¤×˜×™×™×. ×”×—×–×¨ ×ª×•×•×™×ª ××—×ª ×‘×œ×‘×“: "+", ".join(CATEGORIES)

def classify_doc(txt:str)->str:
    for lab,d in CATEGORIES.items():
        if any(re.search(p,txt,re.I) for p in d["regex"]): return lab
    try:
        resp=client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"system","content":CLS_SYSTEM},
                      {"role":"user","content":txt[:1500]}],
            temperature=0,max_tokens=10)
        lab=resp.choices[0].message.content.strip()
    except: lab="××›×ª×‘_××—×¨"
    return lab if lab in CATEGORIES else "××›×ª×‘_××—×¨"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEMPLATES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
H = lambda s: s+"  **×¢× ×” ×‘×¢×‘×¨×™×ª ××œ××” ×•×œ×œ× ××™×œ×™× ×‘×× ×’×œ×™×ª. ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§ ××• ×¤×¡×´×“) ××—×¨×™ ×›×œ ×§×‘×™×¢×”.**"
PROMPTS={
 "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ":dict(summary=H("×¡×›× ××›×ª×‘ ×¤×™×˜×•×¨×™×Ÿ: 1. ×¤×¨×˜×™ ×¢×•×‘×“ ×•×ª××¨×™×›×™×, 2. ×–×›×•×™×•×ª ×•×ª×©×œ×•××™×, 3. ×¦×¢×“×™× ××•××œ×¦×™×."),
                      answer =H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. ×”×©×‘ ×¨×§ ×¢×œ ×¡××š ×”××›×ª×‘ ×•×—×•×§×™ ×¢×‘×•×“×” ×¨×œ×•×•× ×˜×™×™×.")),
 "×—×•×–×”_×¢×‘×•×“×”":  dict(summary=H("×¡×›× ×—×•×–×” ×¢×‘×•×“×”: 1. ×ª× ××™ ×”×¢×¡×§×”, 2. ×¡×¢×™×¤×™ ×¡×•×“×™×•×ª ×•××™-×ª×—×¨×•×ª, 3. ×¡×™×›×•× ×™× ×•×”××œ×¦×•×ª."),
                      answer =H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. × ×ª×— ××ª ×¡×¢×™×¤×™ ×”×—×•×–×”.")),
 "NDA":          dict(summary=H("×¡×›× NDA: 1. ×”×’×“×¨×•×ª ××™×“×¢ ×—×¡×•×™, 2. ×ª×§×•×¤×ª ×—×™×¡×™×•×Ÿ, 3. ×××¦×¢×™ ××›×™×¤×”."),
                      answer =H("××ª×” ×¢×•\"×“ ×§× ×™×™×Ÿ-×¨×•×—× ×™.")),
 "CEASE_DESIST": dict(summary=H("×¡×›× ××›×ª×‘ ××–×”×¨×”: 1. ×˜×¢× ×•×ª, 2. ×“×¨×™×©×•×ª, 3. ×œ×•×—×•×ª ×–×× ×™× ×œ××›×™×¤×”."),
                      answer =H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”.")),
 "×ª×§× ×•×Ÿ":        dict(summary=H("×¡×›× ×ª×§× ×•×Ÿ/××“×™× ×™×•×ª: 1. ××˜×¨×•×ª, 2. ×–×›×•×™×•×ª/×—×•×‘×•×ª, 3. ×¡×™×›×•× ×™× ×œ××™-×¦×™×•×ª."),
                      answer =H("××ª×” ×¢×•\"×“ ×—×‘×¨×•×ª.")),
 "×›×ª×‘_×ª×‘×™×¢×”":    dict(summary=H("×¡×›× ×›×ª×‘ ×ª×‘×™×¢×”: 1. ×¢×™×œ×•×ª, 2. ×¡×¢×“×™×, 3. ×œ×•×— ×–×× ×™× ×“×™×•× ×™."),
                      answer =H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”.")),
 "×¤×¡×§_×“×™×Ÿ":      dict(summary=H("×¡×›× ×¤×¡×§-×“×™×Ÿ: 1. ×©××œ×” ××©×¤×˜×™×ª, 2. ×§×‘×™×¢×•×ª, 3. ×”×œ×›×”."),
                      answer =H("××ª×” ×¢×•\"×“.")),
 "_":            dict(summary=H("×¡×›× ××ª ×”××¡××š: ×ª×§×¦×™×¨, × ×§×•×“×•×ª ×¢×™×§×¨×™×•×ª, ×”×©×œ×›×•×ª."),
                      answer =H("××ª×” ×¢×•\"×“."))
}
tmpl=lambda l,k: PROMPTS.get(l,PROMPTS["_"])[k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RETRIEVAL (RAG) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embed=lambda t: model.encode([t],normalize_embeddings=True)[0]

async def retrieve(query,doc):
    q_emb=embed(query); secs=[embed(c) for c in chunk_text(doc)] if doc else []
    cand={"law":{}, "judg":{}}

    async def add(m,kind):
        meta,score=m.get("metadata",{}), m.get("score",0)
        key="IsraelLawID" if kind=="law" else "CaseNumber"; _id=meta.get(key)
        if not _id: return
        coll=law_collection if kind=="law" else judgment_collection
        d=coll.find_one({key:_id}); 
        if d: cand[kind].setdefault(_id,{"doc":d,"scores":[]})["scores"].append(score)

    async def scan(e):
        rl,rj=await asyncio.gather(
            asyncio.to_thread(law_index.query,vector=e.tolist(),top_k=1,include_metadata=True),
            asyncio.to_thread(judgment_index.query,vector=e.tolist(),top_k=1,include_metadata=True))
        [await add(m,"law") for m in rl.get("matches",[])]
        [await add(m,"judg")for m in rj.get("matches",[])]

    await asyncio.gather(*(scan(e) for e in secs))
    for m in law_index.query(vector=q_emb.tolist(),top_k=3,include_metadata=True).get("matches",[]):  await add(m,"law")
    for m in judgment_index.query(vector=q_emb.tolist(),top_k=3,include_metadata=True).get("matches",[]): await add(m,"judg")

    top=lambda d: sorted(d.values(), key=lambda x:-np.mean(x["scores"]))[:3]
    return [x["doc"] for x in top(cand["law"])],[x["doc"] for x in top(cand["judg"])]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SELF-CHECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def citations_ok(ans:str)->bool:
    if contains_english(ans): return False
    try:
        r=await client_async_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"user","content":"Does every claim have an explicit citation? Answer Yes/No.\n"+ans}],
            temperature=0,max_tokens=3)
        return "yes" in r.choices[0].message.content.lower()
    except: return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CHAT ASSISTANT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_assistant():
    st.markdown('<div class="chat-header">ğŸ’¬ Ask Mini Lawyer</div>', unsafe_allow_html=True)

    # ---------- bootstrap session ----------
    if "cid" not in st.session_state:
        cid = ls_get("AMLChatId") or str(uuid.uuid4())
        ls_set("AMLChatId", cid)
        st.session_state.cid = cid

    if "messages" not in st.session_state:
        conv = conversation_coll.find_one({"local_storage_id": st.session_state.cid})
        st.session_state["messages"] = conv.get("messages", []) if conv else []

    # ---------- restore user name from localStorage (if exists) ----------
    if "user_name" not in st.session_state:
        stored_name = ls_get("AMLUserName")
        if stored_name:
            st.session_state["user_name"] = stored_name

    # ---------- name form (only once) ----------
    if "user_name" not in st.session_state:
        with st.form("name_form"):
            st.text_input("×”×›× ×¡ ×©× ×œ×”×ª×—×œ×ª ×©×™×—×”:", key="user_name_input")
            submitted = st.form_submit_button("×”×ª×—×œ")

        if submitted and st.session_state.get("user_name_input"):
            st.session_state["user_name"] = st.session_state["user_name_input"]      # â† ×›×ª×™×‘×” ×¤×¢× ××—×ª
            ls_set("AMLUserName", st.session_state["user_name"])                     # ×œ×©××™×¨×” ×‘×™×Ÿ ×¨×™×¦×•×ª
            add_msg("assistant", f"×©×œ×•× {st.session_state['user_name']}, ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?")
            conversation_coll.update_one(
                {"local_storage_id": st.session_state.cid},
                {"$set": {"user_name": st.session_state["user_name"],
                          "messages": st.session_state["messages"]}},
                upsert=True
            )
            st.rerun()
        return   # ××—×›×™× ×œ×©×

    # ---------- chat history ----------
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    show_msgs()
    st.markdown("</div>", unsafe_allow_html=True)

    # ---------- file upload ----------
    up = st.file_uploader("ğŸ“„ ×”×¢×œ×” ××¡××š", type=["pdf", "docx"])
    if up:
        raw_txt = read_pdf(up) if up.type == "application/pdf" else read_docx(up)
        st.session_state.doctype = classify_doc(raw_txt)
        st.session_state.doc = "\n".join(l for l in raw_txt.splitlines() if heb.search(l))
        st.success(f"×¡×•×’ ×”××¡××š: {st.session_state.doctype}")

    # ---------- summary ----------
    if hasattr(st.session_state, "doc") and st.button("ğŸ“‹ ×¡×™×›×•×"):
        with st.spinner("×¡×™×›×•×..."):
            prompt = tmpl(st.session_state.doctype, "summary") + "\n" + st.session_state.doc
            r = asyncio.run(
                client_async_openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=700,
                )
            )
            st.session_state.summary = ensure_hebrew(r.choices[0].message.content.strip())

    if st.session_state.get("summary"):
        st.markdown("### ×¡×™×›×•×:")
        st.markdown(
            f"<div dir='rtl' style='text-align:right;line-height:1.6'>{st.session_state.summary}</div>",
            unsafe_allow_html=True,
        )

    # ---------- async answer helpers ----------
    async def generate_answer(q):
        laws, judg = await retrieve(q, st.session_state.get("doc", ""))
        law_txt = "\n\n".join(d.get("Description", "")[:800] for d in laws) or "×œ× × ××¦××• ×—×•×§×™× ×¨×œ×•×•× ×˜×™×™×."
        jud_txt = "\n\n".join(d.get("Description", "")[:800] for d in judg) or "×œ× × ××¦××• ×¤×¡×§×™ ×“×™×Ÿ ×¨×œ×•×•× ×˜×™×™×."
        sys = (
            tmpl(st.session_state.get("doctype", "_"), "answer")
            + f"\n\n--- ××¡××š ---\n{st.session_state.get('doc', '')[:1500]}"
            + f"\n\n--- ×—×•×§×™× ---\n{law_txt}"
            + f"\n\n--- ×¤×¡×§×™ ×“×™×Ÿ ---\n{jud_txt}"
        )
        r = await client_async_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": q}],
            temperature=0,
            max_tokens=700,
        )
        return r.choices[0].message.content.strip()

    async def handle(q):
        ans = ensure_hebrew(await generate_answer(q))
        if await citations_ok(ans):
            return ans
        harder = "×—×•×‘×” ×œ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§/×¤×¡×´×“) ××—×¨×™ ×›×œ ××©×¤×˜."
        ans2 = ensure_hebrew(await generate_answer(q + "\n" + harder))
        return ans2

    # ---------- ask form ----------
    with st.form("ask", clear_on_submit=True):
        q = st.text_area("×”×§×œ×“ ×©××œ×” ××©×¤×˜×™×ª:", height=100)
        send = st.form_submit_button("×©×œ×—")

    if send and q.strip():
        ans = asyncio.run(handle(q.strip()))
        add_msg("user", q.strip())
        add_msg("assistant", ans)
        conversation_coll.update_one(
            {"local_storage_id": st.session_state.cid},
            {"$set": {"messages": st.session_state["messages"],
                      "user_name": st.session_state["user_name"]}},
            upsert=True,
        )
        st.rerun()

    # ---------- clear chat ----------
    if st.button("ğŸ—‘ × ×§×”"):
        conversation_coll.delete_one({"local_storage_id": st.session_state.cid})
        st_js("localStorage.clear()")
        st.session_state.clear()
        st.rerun()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LEGAL FINDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_document_details(kind, doc_id):
    coll = judgment_collection if kind=="Judgment" else law_collection
    key  = "CaseNumber" if kind=="Judgment" else "IsraelLawID"
    return coll.find_one({key:doc_id})


def get_explanation(scenario, doc, kind):
    name = doc.get("Name", "")
    desc = doc.get("Description", "")

    if kind == "Judgment":
        prompt = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:
{scenario}

×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×¤×¡×§ ×”×“×™×Ÿ ×”×‘×:
×©×: {name}
×ª×™××•×¨: {desc}

×× × ×”×¡×‘×¨ ×‘×¦×•×¨×” ×ª××¦×™×ª×™×ª ×•××§×¦×•×¢×™×ª ××“×•×¢ ×¤×¡×§ ×“×™×Ÿ ×–×” ×™×›×•×œ ×œ×¢×–×•×¨ ×œ××§×¨×” ×–×”,
×•×”×¢×¨×š ××•×ª×• ×‘×¡×•×œ× 0-10 (0 = ×œ× ×¢×•×–×¨ ×›×œ×œ, 10 = ××ª××™× ×‘××“×•×™×§).
**××œ ×ª×™×ª×Ÿ ×œ×¨×•×‘ ×”××¡××›×™× ×¦×™×•×Ÿ 9 â€“ ×”×™×” ××’×•×•×Ÿ!**
×”×—×–×¨ JSON ×‘×œ×‘×“, ×œ×“×•×’××”:
{{
  "advice": "×”×¡×‘×¨ ××§×¦×•×¢×™ ×‘×¢×‘×¨×™×ª",
  "score": 8
}}
××™×Ÿ ×œ×”×•×¡×™×£ ×˜×§×¡×˜ × ×•×¡×£.
"""
    else:  # kind == "Law"
        prompt = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:
{scenario}

×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×”×—×•×§ ×”×‘×:
×©×: {name}
×ª×™××•×¨: {desc}

×× × ×”×¡×‘×¨ ×‘×¦×•×¨×” ×ª××¦×™×ª×™×ª ×•××§×¦×•×¢×™×ª ××“×•×¢ ×—×•×§ ×–×” ×™×›×•×œ ×œ×¢×–×•×¨ ×œ××§×¨×” ×–×”,
×•×”×¢×¨×š ××•×ª×• ×‘×¡×•×œ× 0-10 (0 = ×œ× ×§×©×•×¨, 10 = ××ª××™× ×›××• ×›×¤×¤×”).
**××œ ×ª×™×ª×Ÿ ×œ×¨×•×‘ ×”×—×•×§×™× ×¦×™×•×Ÿ 9 â€“ ×”×™×” ××’×•×•×Ÿ!**
×”×—×–×¨ JSON ×‘×œ×‘×“, ×œ×“×•×’××”:
{{
  "advice": "×”×¡×‘×¨ ×ª××¦×™×ª×™ ×•××§×¦×•×¢×™ ×‘×¢×‘×¨×™×ª",
  "score": 7
}}
××™×Ÿ ×œ×”×•×¡×™×£ ×˜×§×¡×˜ × ×•×¡×£.
"""

    try:
        response = client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )
        return json.loads(response.choices[0].message.content.strip())
    except Exception as e:
        st.error(f"Error from GPT: {e}")
        return {"advice": "×œ× × ×™×ª×Ÿ ×œ×§×‘×œ ×”×¡×‘×¨ ×‘×©×œ×‘ ×–×”.", "score": "N/A"}



def legal_finder_assistant():
    st.title("Legal Finder Assistant")

    kind = st.selectbox("Choose what to search", ["Judgment", "Law"])
    scen = st.text_area("Describe your scenario")

    if st.button("Find Suitable Results") and scen:
        # ---------- Pinecone similarity search ---------------------------------------
        q_emb  = model.encode([scen], normalize_embeddings=True)[0]
        index  = judgment_index if kind == "Judgment" else law_index
        id_key = "CaseNumber" if kind == "Judgment" else "IsraelLawID"

        res     = index.query(vector=q_emb.tolist(), top_k=5, include_metadata=True)
        matches = res.get("matches", [])
        if not matches:
            st.info("No matches found.")
            return

        # ---------- loop over matches ----------------------------------------
        for m in matches:
            doc_id = m.get("metadata", {}).get(id_key)
            if not doc_id:
                continue
            doc = load_document_details(kind, doc_id)
            if not doc:
                continue

            name     = doc.get("Name", "No Name")
            desc     = doc.get("Description", "N/A")
            date_lbl = "DecisionDate" if kind == "Judgment" else "PublicationDate"

            extra_html = (
                f"<div class='law-meta'>Procedure Type: {doc.get('ProcedureType','N/A')}</div>"
                if kind == "Judgment" else ""
            )

            st.markdown(
                f"<div class='law-card'>"
                f"<div class='law-title'>{name} (ID: {doc_id})</div>"
                f"<div class='law-description'>{desc}</div>"
                f"<div class='law-meta'>{date_lbl}: {doc.get(date_lbl, 'N/A')}</div>"
                f"{extra_html}"
                f"</div>",
                unsafe_allow_html=True
            )

            # ---------- GPT Advise (Hidden Score) -----------------------------
            with st.spinner("Getting explanation..."):
                result = get_explanation(scen, doc, kind)

            advice = result.get("advice", "")
            # score  = result.get("score", "N/A") 

            st.markdown(
                f"<span style='color:red;'>×¢×¦×ª ×”××ª×¨: {advice}</span>",
                unsafe_allow_html=True
            )

            # ---------- full JSON toggle -------------------------------
            with st.expander(f"View Full Details for {doc_id}"):
                st.json(doc)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if app_mode=="Chat Assistant":
    chat_assistant()
else:
    legal_finder()
