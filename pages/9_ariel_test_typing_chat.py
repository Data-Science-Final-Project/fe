# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, re, json, uuid, asyncio
from datetime import datetime

import streamlit as st
import torch, fitz, docx, numpy as np
from bidi.algorithm import get_display
from dotenv import load_dotenv
from openai import AsyncOpenAI, OpenAI
from streamlit_js import st_js, st_js_blocking

from app_resources import mongo_client, pinecone_client, model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ env / clients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPEN_AI")
DATABASE_NAME  = os.getenv("DATABASE_NAME")

client_async_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)
client_sync_openai  = OpenAI(api_key=OPENAI_API_KEY)

judgment_index = pinecone_client.Index("judgments-names")
law_index      = pinecone_client.Index("laws-names")

db = mongo_client[DATABASE_NAME]
judgment_coll = db["judgments"]
law_coll      = db["laws"]
conv_coll     = db["conversations"]

torch.classes.__path__ = []
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("Ask Mini Lawyer", "âš–ï¸", layout="wide")
st.markdown("""
<style>
.chat-container{background:#1E1E1E;padding:20px;border-radius:10px}
.chat-header{color:#4CAF50;font-size:36px;font-weight:bold;text-align:center}
.user-message{background:#4CAF50;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.bot-message{background:#44475a;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.timestamp{font-size:0.75em;color:#bbb}
.law-card{border:1px solid #e0e0e0;border-radius:10px;padding:20px;margin-bottom:15px;
          box-shadow:0 2px 4px rgba(0,0,0,0.1);background:#f9f9f9}
.law-title{font-size:20px;font-weight:bold;color:#333}
.law-description{font-size:16px;color:#444;margin:10px 0}
.law-meta{font-size:14px;color:#555}
.stButton>button{background:#7ce38b;color:#fff;font-size:14px;border:none;padding:8px 16px;border-radius:5px;cursor:pointer}
.stButton>button:hover{background:#69d67a}
</style>
""", unsafe_allow_html=True)

app_mode = st.sidebar.selectbox("Choose module", ["Chat Assistant", "Legal Finder"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# localStorage
def ls_get(key: str) -> str | None:
    with st.container():
        st.markdown("<div style='display:none'>", unsafe_allow_html=True)
        value = st_js_blocking(f"return localStorage.getItem('{key}');", key=f"ls_{key}")
        st.markdown("</div>", unsafe_allow_html=True)
    return value

def ls_set(key: str, value: str) -> None:
    st_js(f"localStorage.setItem('{key}', '{value}');")

# RTL text normalization
heb = re.compile(r"[×-×ª]")  
SALUT = re.compile(r"\b(×œ×›×‘×•×“|××¨|××¨\.?|×’×‘'?|×’×‘×¨×ª|×“\"×¨|×“\"×¨\.?)\b")

def rtl_norm(text: str) -> str:
    if not heb.search(text):
        return text
    try:
        text = get_display(text)
    except Exception:
        text = " ".join(word[::-1] if heb.search(word) else word for word in text.split())  
    text = SALUT.sub("", text)
    return re.sub(r"\s{2,}", " ", text).strip()

# File readers
read_pdf = lambda f: "".join(rtl_norm(p.get_text()) for p in fitz.open(stream=f.read(), filetype="pdf"))
read_docx = lambda f: "\n".join(rtl_norm(p.text) for p in docx.Document(f).paragraphs if p.text.strip())

# Chat UI
def add_msg(role: str, text: str) -> None:
    st.session_state.setdefault("messages", []).append({
        "role": role,
        "content": text,
        "timestamp": datetime.now().strftime("%H:%M:%S")
    })

def show_msgs() -> None:
    for msg in st.session_state.get("messages", []):
        css_class = "user-message" if msg["role"] == "user" else "bot-message"
        st.markdown(
            f"<div class='{css_class}'>{msg['content']}<div class='timestamp'>{msg['timestamp']}</div></div>",
            unsafe_allow_html=True,
        )

# Text processing
def chunk_text(text: str, max_len: int = 450) -> list[str]:
    if not isinstance(text, str):
        text = str(text)
    sentences = re.split(r"(?:\.|\?|!)\s+", text)
    chunks, current = [], ""
    for sentence in sentences:
        if len(current) + len(sentence) > max_len and current:
            chunks.append(current.strip())
            current = sentence
        else:
            current += " " + sentence
    if current.strip():
        chunks.append(current.strip())
    return chunks[:20]

contains_en = lambda s: bool(re.search(r"[A-Za-z]", s))

def ensure_he(text: str) -> str:
    english_word_count = sum(contains_en(word) for word in text.split())
    if english_word_count <= 3:
        return text
    response = client_sync_openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "×ª×¨×’× ×œ×¢×‘×¨×™×ª ××œ××”:\n" + text}],
        temperature=0,
        max_tokens=len(text) // 2,
    )
    return response.choices[0].message.content.strip()




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOC_LABELS = {
    "×—×•×–×”_×¢×‘×•×“×”":  "×”×¡×›× ×‘×™×Ÿ ××¢×¡×™×§ ×œ×¢×•×‘×“ ××• ××•×¢××“ ×œ×¢×‘×•×“×”",
    "×—×•×–×”_×©×›×™×¨×•×ª": "×”×¡×›× ×œ×”×©×›×¨×ª ×“×™×¨×”, ××©×¨×“ ××• × ×›×¡ ××—×¨",
    "×—×•×–×”_×©×™×¨×•×ª":  "×”×¡×›× ×‘×™×Ÿ ××–××™×Ÿ ×œ×¡×¤×§ ×©×™×¨×•×ª",
    "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ": "×”×•×“×¢×” ×¢×œ ×¡×™×•× ×”×¢×¡×§×” ××• ×”×¤×¡×§×ª ×¢×‘×•×“×”",
    "××›×ª×‘_×”×ª×¨××”":  "××›×ª×‘ ×“×¨×™×©×” ××• ××–×”×¨×” ×œ×¤× ×™ × ×§×™×˜×ª ×”×œ×™×›×™×",
    "×ª×§× ×•×Ÿ":       "××¡××š ×›×œ×œ×™ ×—×•×‘×•×ª ×•×–×›×•×™×•×ª",
    "NDA":         "×”×¡×›× ×¡×•×“×™×•×ª ×•××™-×’×™×œ×•×™",
    "×›×ª×‘_×ª×‘×™×¢×”":   "××¡××š ×¤×ª×™×—×ª ×”×œ×™×š ×‘×‘×™×ª-××©×¤×˜",
    "×›×ª×‘_×”×’× ×”":    "×ª×’×•×‘×” ×œ×›×ª×‘ ×ª×‘×™×¢×”",
    "×¤×¡×§_×“×™×Ÿ":     "×”×›×¨×¢×ª ×‘×™×ª-××©×¤×˜",
    "××¡××š_××—×¨":    "×›×œ ××¡××š ××©×¤×˜×™ ××—×¨"
}

CLS_EXAMPLES = [
    ("×”× × ×• ×œ×”×•×“×™×¢×š ×‘×–××ª ×›×™ ×”×¢×¡×§×ª×š ×ª×¡×ª×™×™× ×‘×ª××¨×™×š â€¦", "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ"),
    ("×”×¢×•×‘×“ ××ª×—×™×™×‘ ×œ×©××•×¨ ×‘×¡×•×“ ×›×œ ××™×“×¢",              "×—×•×–×”_×¢×‘×•×“×”"),
    ("×”×©×•×›×¨ ××ª×—×™×™×‘ ×œ×”×—×–×™×¨ ××ª ×”× ×›×¡ ×›×©×”×•× × ×§×™",        "×—×•×–×”_×©×›×™×¨×•×ª"),
    ("×”×¦×“×“×™× ××¡×›×™××™× ×©×œ× ×œ×’×œ×•×ª ××™×“×¢ ×¡×•×“×™",           "NDA"),
    ("×”× ×ª×‘×¢ ×‘×™×¦×¢ ×¨×©×œ× ×•×ª â€¦ ×œ×¤×™×›×š ××ª×‘×§×© ×‘×™×”××´×©",        "×›×ª×‘_×ª×‘×™×¢×”")
]

CLS_SYS = (
    "××ª×” ××¡×•×•×’ ××¡××›×™× ××©×¤×˜×™×™×. ×”×—×–×¨ JSON ×‘××‘× ×”:\n"
    '{"label":"<LABEL>","confidence":0-100}\n'
    f"×¢×œ×™×š ×œ×‘×—×•×¨ ×¨×§ ××ª×•×š: {', '.join(DOC_LABELS.keys())}."
)

def classify_doc(txt: str) -> str:
    # Ensure txt is a string to prevent crashes (e.g., from None or file objects)
    if not isinstance(txt, str):
        try:
            txt = txt.decode("utf-8")
        except Exception:
            txt = str(txt)

    chunks = chunk_text(txt, max_len=500)[:3] or [txt[:1500]]
    msgs = [{"role": "system", "content": CLS_SYS}]
    
    for eg, lbl in CLS_EXAMPLES:
        msgs += [
            {"role": "user", "content": eg},
            {"role": "assistant", "content": json.dumps({"label": lbl, "confidence": 95})}
        ]
    
    for c in chunks:
        msgs.append({"role": "user", "content": c})

    try:
        r = client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=msgs,
            temperature=0,
            max_tokens=20
        )
        j = json.loads(r.choices[0].message.content)
        return j["label"] if j.get("label") in DOC_LABELS else "××¡××š_××—×¨"
    except Exception:
        return "××¡××š_××—×¨"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
H = lambda s: s + "  **×¢× ×” ×‘×¢×‘×¨×™×ª ××œ××” ×•×œ×œ× ××™×œ×™× ×‘×× ×’×œ×™×ª. ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§ ××• ×¤×¡×´×“) ××—×¨×™ ×›×œ ×§×‘×™×¢×”.**"
PROMPTS = {
    "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ": dict(
        summary=H("×¡×›× ××›×ª×‘ ×¤×™×˜×•×¨×™×Ÿ: 1. ×¤×¨×˜×™ ×¢×•×‘×“ ×•×ª××¨×™×›×™×, 2. ×–×›×•×™×•×ª ×•×ª×©×œ×•××™×, 3. ×¦×¢×“×™× ××•××œ×¦×™×."),
        answer=H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. ×”×©×‘ ×¨×§ ×¢×œ ×¡××š ×”××›×ª×‘ ×•×—×•×§×™ ×¢×‘×•×“×” ×¨×œ×•×•× ×˜×™×™×.")
    ),
    "×—×•×–×”_×¢×‘×•×“×”": dict(
        summary=H("×¡×›× ×—×•×–×” ×¢×‘×•×“×”: 1. ×ª× ××™ ×”×¢×¡×§×”, 2. ×¡×¢×™×¤×™ ×¡×•×“×™×•×ª ×•××™-×ª×—×¨×•×ª, 3. ×¡×™×›×•× ×™× ×•×”××œ×¦×•×ª."),
        answer=H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. × ×ª×— ××ª ×¡×¢×™×¤×™ ×”×—×•×–×”.")
    ),
    "×ª×§× ×•×Ÿ": dict(
        summary=H("×¡×›× ×ª×§× ×•×Ÿ/××“×™× ×™×•×ª: 1. ××˜×¨×•×ª, 2. ×–×›×•×™×•×ª/×—×•×‘×•×ª, 3. ×¡×™×›×•× ×™× ×œ××™-×¦×™×•×ª."),
        answer=H("××ª×” ×¢×•\"×“ ×—×‘×¨×•×ª. ×”×¡×‘×¨ ×ª×•×§×£ ×¡×¢×™×¤×™ ×”×ª×§× ×•×Ÿ.")
    ),
    "×›×ª×‘_×ª×‘×™×¢×”": dict(
        summary=H("×¡×›× ×›×ª×‘ ×ª×‘×™×¢×”: 1. ×¢×™×œ×•×ª, 2. ×¡×¢×“×™×, 3. ×œ×•×— ×–×× ×™× ×“×™×•× ×™."),
        answer=H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”. × ×ª×— ××ª ×¢×™×œ×•×ª ×”×ª×‘×™×¢×”.")
    ),
    "×¤×¡×§_×“×™×Ÿ": dict(
        summary=H("×¡×›× ×¤×¡×§-×“×™×Ÿ: 1. ×©××œ×” ××©×¤×˜×™×ª, 2. ×§×‘×™×¢×•×ª, 3. ×”×œ×›×”."),
        answer=H("××ª×” ×¢×•\"×“. ×”×¡×‘×¨ ××ª ×”×œ×›×ª ×‘×™×ª-×”××©×¤×˜.")
    ),
    "_": dict(
        summary=H("×¡×›× ××ª ×”××¡××š: ×ª×§×¦×™×¨, × ×§×•×“×•×ª ×¢×™×§×¨×™×•×ª, ×”×©×œ×›×•×ª."),
        answer=H("××ª×” ×¢×•\"×“.")
    )
}
tmpl = lambda l, k: PROMPTS.get(l, PROMPTS["_"])[k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ retrieval (RAG) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embed = lambda t: model.encode([t], normalize_embeddings=True)[0]

async def retrieve(q, doc):
    q_emb = embed(q)
    secs = [embed(c) for c in chunk_text(doc, 400)] if doc else []
    cand = {"law": {}, "judg": {}}

    async def add(match, kind):
        meta, score = match.get("metadata", {}), match.get("score", 0)
        key = "IsraelLawID" if kind == "law" else "CaseNumber"
        _id = meta.get(key)
        if not _id:
            return
        coll = law_coll if kind == "law" else judgment_coll
        d = coll.find_one({key: _id})
        if d:
            cand[kind].setdefault(_id, {"doc": d, "scores": []})["scores"].append(score)

    async def query(idx, vec):
        vec_list = vec.tolist() if isinstance(vec, np.ndarray) else vec
        return idx.query(
            vector=vec_list,
            top_k=5,
            include_metadata=True
        ).get("matches", [])

    async def scan(vec):
        for m in await query(law_index, vec):
            await add(m, "law")
        for m in await query(judgment_index, vec):
            await add(m, "judg")

    await asyncio.gather(scan(q_emb), *(scan(e) for e in secs[:10]))

    top = lambda d: sorted(d.values(), key=lambda x: -np.mean(x["scores"]))[:3]
    return [x["doc"] for x in top(cand["law"])], [x["doc"] for x in top(cand["judg"])]

async def citations_ok(ans: str) -> bool:
    pat = r'\[\d+\]|\(\d+\)'
    lines = [l.strip() for l in ans.splitlines() if l.strip()]
    return all(re.search(pat, l) for l in lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ gen â€“ ×ª×©×•×‘×” ××©×¤×˜×™×ª ××§×¦×•×¢×™×ª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def gen(q: str) -> str:
    laws, judg = await retrieve(q, st.session_state.get("doc", ""))

    SNIPPET = 400
    def fmt_sources(lst, tag):
        if not lst:
            return f"×œ× × ××¦××• {tag} ×¨×œ×•×•× ×˜×™×™×."
        return "\n".join(
            f"[{i}] {d.get('Name', '×œ×œ× ×©×')} â€“ {(d.get('Description', '') or '')[:SNIPPET].strip()}"
            for i, d in enumerate(lst, 1)
        )

    sys = (
        tmpl(st.session_state.get("doctype", "_"), "answer") +
        "\n\n×”× ×—×™×•×ª × ×™×¡×•×— (×—×•×‘×”):\n"
        "â€¢ ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ××œ××” ×‘×œ×‘×“ â€“ ××™×Ÿ ×œ×”×©×ª××© ×‘×× ×’×œ×™×ª.\n"
        "â€¢ ×”×©×ª××© ×‘×œ×©×•×Ÿ ××©×¤×˜×™×ª-××§×¦×•×¢×™×ª, ×¤×¡×§××•×ª/×¡×¢×™×¤×™× ×××•×¡×¤×¨×™×.\n"
        "â€¢ ×”×¡×ª××š ××š ×•×¨×§ ×¢×œ ×”××§×•×¨×•×ª ×©×œ××˜×”, ×•×¦×™×™×Ÿ ×‘×¡×•×’×¨×™×™× ××ª ××¡×¤×¨-×”××§×•×¨ ×œ×™×“ ×›×œ ×§×‘×™×¢×”.\n"
        f"\n--- ××¡××š ---\n{st.session_state.get('doc', '')[:1000]}" +
        f"\n\n--- ×—×•×§×™× ---\n{fmt_sources(laws, '×—×•×§×™×')}" +
        f"\n\n--- ×¤×¡×§×™ ×“×™×Ÿ ---\n{fmt_sources(judg, '×¤×¡×§×™ ×“×™×Ÿ')}"
    )

    r = await client_async_openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": sys},
            {"role": "user", "content": q}
        ],
        temperature=0,
        max_tokens=900
    )
    return r.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ chat assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_assistant():
    st.markdown('<div class="chat-header">ğŸ’¬ Ask Mini Lawyer</div>', unsafe_allow_html=True)

    # â”€â”€â”€â”€â”€ cid & messages â”€â”€â”€â”€â”€
    if "cid" not in st.session_state:
        cid = ls_get("AMLChatId") or str(uuid.uuid4())
        ls_set("AMLChatId", cid)
        st.session_state.cid = cid

    if "messages" not in st.session_state:
        conv = conv_coll.find_one({"local_storage_id": st.session_state.cid})
        st.session_state["messages"] = conv.get("messages", []) if conv else []

    # â”€â”€â”€â”€â”€ user name â”€â”€â”€â”€â”€
    if "user_name" not in st.session_state:
        stored = ls_get("AMLUserName")
        if stored:
            st.session_state["user_name"] = stored

    if "user_name" not in st.session_state:
        with st.form("name"):
            st.text_input("×”×›× ×¡ ×©× ×œ×”×ª×—×œ×ª ×©×™×—×”:", key="user_name_input")
            sub = st.form_submit_button("×”×ª×—×œ")
        if sub and st.session_state.get("user_name_input"):
            st.session_state["user_name"] = st.session_state["user_name_input"]
            ls_set("AMLUserName", st.session_state["user_name"])
            add_msg("assistant", f"×©×œ×•× {st.session_state['user_name']}, ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?")
            conv_coll.update_one(
                {"local_storage_id": st.session_state.cid},
                {"$set": {
                    "user_name": st.session_state["user_name"],
                    "messages": st.session_state["messages"]
                }},
                upsert=True
            )
            st.rerun()
        return  # ×××ª×™×Ÿ ×œ×”×’×“×¨×ª ×©×

    # â”€â”€â”€â”€â”€ history â”€â”€â”€â”€â”€
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    show_msgs()
    st.markdown("</div>", unsafe_allow_html=True)

    # â”€â”€â”€â”€â”€ upload â”€â”€â”€â”€â”€
    up = st.file_uploader("ğŸ“„ ×”×¢×œ×” ××¡××š", type=["pdf", "docx"])
    if up:
        raw = read_pdf(up) if up.type == "application/pdf" else read_docx(up)
        st.session_state.doctype = classify_doc(raw)
        st.session_state.doc = "\n".join(l for l in raw.splitlines() if heb.search(l))
        st.success(f"×¡×•×’ ×”××¡××š: {st.session_state.doctype}")

    # â”€â”€â”€â”€â”€ summary â”€â”€â”€â”€â”€
    if hasattr(st.session_state, "doc") and st.button("ğŸ“‹ ×¡×™×›×•×"):
        with st.spinner("×¡×™×›×•×..."):
            prompt = (
                tmpl(st.session_state.doctype, "summary") +
                "\n\n×›×ª×•×‘ 4-6 Bullet-×™× ×§×¦×¨×™× (×¢×“ 30 ××™×œ×™× ×›×œ ××—×“):\n" +
                st.session_state.doc[:2000]
            )
            r = asyncio.run(
                client_async_openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=350
                )
            )
            st.session_state.summary = ensure_he(
                r.choices[0].message.content.strip().replace("â€¢", "â€“")
            )

    if st.session_state.get("summary"):
        st.markdown("### ×¡×™×›×•×:")
        st.markdown(
            f"<div dir='rtl' style='text-align:right'>{st.session_state.summary}</div>",
            unsafe_allow_html=True
        )

    # â”€â”€â”€â”€â”€ answer helpers â”€â”€â”€â”€â”€
    async def handle(q):
        ans = ensure_he(await gen(q))
        if await citations_ok(ans):
            return ans
        ans2 = ensure_he(
            await gen(q + "\n×—×•×‘×” ×œ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§/×¤×¡\"×“) ××—×¨×™ ×›×œ ××©×¤×˜.")
        )
        return ans2

    # â”€â”€â”€â”€â”€ ask form â”€â”€â”€â”€â”€
    with st.form("ask", clear_on_submit=True):
        q = st.text_area("×”×§×œ×“ ×©××œ×” ××©×¤×˜×™×ª:", height=100)
        send = st.form_submit_button("×©×œ×—")

    if send and q.strip():
        ans = asyncio.run(handle(q.strip()))
        add_msg("user", q.strip())
        add_msg("assistant", ans)
        conv_coll.update_one(
            {"local_storage_id": st.session_state.cid},
            {"$set": {
                "messages": st.session_state["messages"],
                "user_name": st.session_state["user_name"]
            }},
            upsert=True
        )
        st.rerun()

    # â”€â”€â”€â”€â”€ clear chat â”€â”€â”€â”€â”€
    if st.button("ğŸ—‘ × ×§×” ×©×™×—×”"):
    
    try:
        conv_coll.delete_one({"local_storage_id": st.session_state.cid})
    except Exception as e:
        st.error(f"×©×’×™××” ×‘× ×™×§×•×™ ×”×©×™×—×” ××‘×¡×™×¡ ×”× ×ª×•× ×™×: {e}")
    st_js_blocking("""
        localStorage.removeItem('AMLUserName');
        localStorage.removeItem('AMLChatId');
    """)
    st.session_state.clear()
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ legal finder assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_document_details(kind, doc_id):
    coll = judgment_coll if kind == "Judgment" else law_coll
    key = "CaseNumber" if kind == "Judgment" else "IsraelLawID"
    return coll.find_one({key: doc_id})

def get_explanation(scenario, doc, kind):
    name, desc = doc.get("Name", ""), doc.get("Description", "")
    if kind == "Judgment":
        prom = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:
{scenario}

×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×¤×¡×§ ×”×“×™×Ÿ ×”×‘×:
×©×: {name}
×ª×™××•×¨: {desc}

×”×¡×‘×¨ ×‘×§×¦×¨×” ××“×•×¢ ×¤×¡×§ ×“×™×Ÿ ×–×” ××¡×™×™×¢ ×•×“×¨×’ 0-10.
×”×—×–×¨ JSON ×›××•:
{{"advice":"×”×¡×‘×¨","score":7}}"""
    else:
        prom = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:
{scenario}

×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×”×—×•×§ ×”×‘×:
×©×: {name}
×ª×™××•×¨: {desc}

×”×¡×‘×¨ ×‘×§×¦×¨×” ××“×•×¢ ×”×—×•×§ ×¨×œ×•×•× ×˜×™ ×•×“×¨×’ 0-10.
×”×—×–×¨ JSON ×›××•:
{{"advice":"×”×¡×‘×¨","score":6}}"""
    try:
        r = client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prom}],
            temperature=0.7
        )
        return json.loads(r.choices[0].message.content.strip())
    except Exception:
        return {"advice": "×©×’×™××”", "score": "N/A"}

def legal_finder_assistant():
    st.title("Legal Finder Assistant")
    kind = st.selectbox("Choose what to search", ["Judgment", "Law"])
    scen = st.text_area("Describe your scenario")
    if st.button("Find Suitable Results") and scen:
        q_emb = model.encode([scen], normalize_embeddings=True)[0]
        idx = judgment_index if kind == "Judgment" else law_index
        key = "CaseNumber" if kind == "Judgment" else "IsraelLawID"
        matches = idx.query(
            vector=q_emb.tolist(),
            top_k=5,
            include_metadata=True
        ).get("matches", [])
        if not matches:
            st.info("No matches found.")
            return
        for m in matches:
            _id = m.get("metadata", {}).get(key)
            doc = load_document_details(kind, _id)
            if not doc:
                continue
            name, desc = doc.get("Name", ""), doc.get("Description", "")
            date_lbl = "DecisionDate" if kind == "Judgment" else "PublicationDate"
            extra = (
                f"<div class='law-meta'>Procedure Type: {doc.get('ProcedureType','N/A')}</div>"
                if kind == "Judgment" else ""
            )
            st.markdown(
                f"<div class='law-card'><div class='law-title'>{name} (ID:{_id})</div>"
                f"<div class='law-description'>{desc}</div>"
                f"<div class='law-meta'>{date_lbl}: {doc.get(date_lbl,'N/A')}</div>{extra}</div>",
                unsafe_allow_html=True
            )
            with st.spinner("GPT explanation..."):
                res = get_explanation(scen, doc, kind)
            st.markdown(
                f"<span style='color:red;'>×¢×¦×ª ×”××ª×¨: {res.get('advice','')}</span>",
                unsafe_allow_html=True
            )
            with st.expander(f"View Full Details for {_id}"):
                st.json(doc)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if app_mode == "Chat Assistant":
    chat_assistant()
else:
    legal_finder_assistant()
