
import os, re, json, uuid, asyncio

from datetime import datetime



import streamlit as st

import torch, fitz, docx, numpy as np

from bidi.algorithm import get_display

from dotenv import load_dotenv

from openai import AsyncOpenAI, OpenAI

from streamlit_js import st_js, st_js_blocking



from app_resources import mongo_client, pinecone_client, model



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ env / clients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

load_dotenv()

OPENAI_API_KEY = os.getenv("OPEN_AI")

DATABASE_NAME = os.getenv("DATABASE_NAME")



if not OPENAI_API_KEY:

    st.error("OPEN_AI API key not found in environment variables. Please set it.")

    st.stop() # Stop execution if key is missing



client_async_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)

client_sync_openai = OpenAI(api_key=OPENAI_API_KEY)



try:

    judgment_index = pinecone_client.Index("judgments-names")

    law_index = pinecone_client.Index("laws-names")

except Exception as e:

    st.warning(f"Could not connect to Pinecone indexes: {e}. RAG functionality might be limited.")

    judgment_index = None # Set to None if connection fails

    law_index = None # Set to None if connection fails



db = mongo_client[DATABASE_NAME]

judgment_coll = db["judgments"]

law_coll = db["laws"]

conv_coll = db["conversations"]



torch.classes.__path__ = []

os.environ["TOKENIZERS_PARALLELISM"] = "false"



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config("Ask Mini Lawyer", "âš–ï¸", layout="wide")

st.markdown("""

<style>

.chat-container{background:#1E1E1E;padding:20px;border-radius:10px}

.chat-header{color:#4CAF50;font-size:36px;font-weight:bold;text-align:center}

.user-message{background:#4CAF50;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}

.bot-message{background:#44475a;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}

.timestamp{font-size:0.75em;color:#bbb}

.law-card{border:1px solid #e0e0e0;border-radius:10px;padding:20px;margin-bottom:15px;

          box-shadow:0 2px 4px rgba(0,0,0,0.1);background:#f9f9f9}

.law-title{font-size:20px;font-weight:bold;color:#333}

.law-description{font-size:16px;color:#444;margin:10px 0}

.law-meta{font-size:14px;color:#555}

.stButton>button{background:#7ce38b;color:#fff;font-size:14px;border:none;padding:8px 16px;border-radius:5px;cursor:pointer}

.stButton>button:hover{background:#69d67a}

</style>

""", unsafe_allow_html=True)



app_mode = st.sidebar.selectbox("Choose module", ["Chat Assistant", "Legal Finder"])



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ls_get(key: str) -> str | None:

    with st.container():

        st.markdown("<div style='display:none'>", unsafe_allow_html=True)

        value = st_js_blocking(f"return localStorage.getItem('{key}');", key=f"ls_{key}")

        st.markdown("</div>", unsafe_allow_html=True)

    return value



def ls_set(key: str, value: str) -> None:

    st_js(f"localStorage.setItem('{key}', '{value}');")



heb = re.compile(r"[×-×ª]")

SALUT = re.compile(r"\b(×œ×›×‘×•×“|××¨|××¨\.?|×’×‘'?|×’×‘×¨×ª|×“\"×¨|×“\"×¨\.?)\b")



def rtl_norm(text: str) -> str:

    if not heb.search(text):

        return text

    try:

        text = get_display(text)

    except Exception:

        text = " ".join(word[::-1] if heb.search(word) else word for word in text.split())

    text = SALUT.sub("", text)

    return re.sub(r"\s{2,}", " ", text).strip()



read_pdf = lambda f: "".join(rtl_norm(p.get_text()) for p in fitz.open(stream=f.read(), filetype="pdf"))

read_docx = lambda f: "\n".join(rtl_norm(p.text) for p in docx.Document(f).paragraphs if p.text.strip())



def add_msg(role: str, text: str) -> None:

    st.session_state.setdefault("messages", []).append({

        "role": role,

        "content": text,

        "timestamp": datetime.now().strftime("%H:%M:%S")

    })



def show_msgs() -> None:

    for msg in st.session_state.get("messages", []):

        css_class = "user-message" if msg["role"] == "user" else "bot-message"

        st.markdown(

            f"<div class='{css_class}'>{msg['content']}<div class='timestamp'>{msg['timestamp']}</div></div>",

            unsafe_allow_html=True,

        )



def chunk_text(text: str, max_len: int = 450) -> list[str]:

    if not isinstance(text, str):

        text = str(text)

    sentences = re.split(r"(?:\.|\?|!)\s+", text)

    chunks, current = [], ""

    for sentence in sentences:

        if len(current) + len(sentence) > max_len and current:

            chunks.append(current.strip())

            current = sentence

        else:

            current += " " + sentence

    if current.strip():

        chunks.append(current.strip())

    return chunks[:20]



contains_en = lambda s: bool(re.search(r"[A-Za-z]", s))



def ensure_he(text: str) -> str:

    english_word_count = sum(1 for word in text.split() if contains_en(word))

    total_words = len(text.split())

    if total_words > 0 and (english_word_count / total_words) < 0.2:

        return text

    try:

        response = client_sync_openai.chat.completions.create(

            model="gpt-3.5-turbo",

            messages=[{"role": "user", "content": "×ª×¨×’× ×œ×¢×‘×¨×™×ª ××œ××”:\n" + text}],

            temperature=0,

            max_tokens=len(text) * 2

        )

        return response.choices[0].message.content.strip()

    except Exception as e:

        st.warning(f"Translation failed: {e}. Returning original text.")

        return text



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DOC_LABELS = {

    "×—×•×–×”_×¢×‘×•×“×”": "×”×¡×›× ×‘×™×Ÿ ××¢×¡×™×§ ×œ×¢×•×‘×“ ××• ××•×¢××“ ×œ×¢×‘×•×“×”",

    "×—×•×–×”_×©×›×™×¨×•×ª": "×”×¡×›× ×œ×”×©×›×¨×ª ×“×™×¨×”, ××©×¨×“ ××• × ×›×¡ ××—×¨",

    "×—×•×–×”_×©×™×¨×•×ª": "×”×¡×›× ×‘×™×Ÿ ××–××™×Ÿ ×œ×¡×¤×§ ×©×™×¨×•×ª",

    "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ": "×”×•×“×¢×” ×¢×œ ×¡×™×•× ×”×¢×¡×§×” ××• ×”×¤×¡×§×ª ×¢×‘×•×“×”",

    "××›×ª×‘_×”×ª×¨××”": "××›×ª×‘ ×“×¨×™×©×” ××• ××–×”×¨×” ×œ×¤× ×™ × ×§×™×˜×ª ×”×œ×™×›×™×",

    "×ª×§× ×•×Ÿ": "××¡××š ×›×œ×œ×™ ×—×•×‘×•×ª ×•×–×›×•×™×•×ª",

    "NDA": "×”×¡×›× ×¡×•×“×™×•×ª ×•××™-×’×™×œ×•×™",

    "×›×ª×‘_×ª×‘×™×¢×”": "××¡××š ×¤×ª×™×—×ª ×”×œ×™×š ×‘×‘×™×ª-××©×¤×˜",

    "×›×ª×‘_×”×’× ×”": "×ª×’×•×‘×” ×œ×›×ª×‘ ×ª×‘×™×¢×”",

    "×¤×¡×§_×“×™×Ÿ": "×”×›×¨×¢×ª ×‘×™×ª-××©×¤×˜",

    "××¡××š_××—×¨": "×›×œ ××¡××š ××©×¤×˜×™ ××—×¨"

}



CLS_EXAMPLES = [

    ("×”× × ×• ×œ×”×•×“×™×¢×š ×‘×–××ª ×›×™ ×”×¢×¡×§×ª×š ×ª×¡×ª×™×™× ×‘×ª××¨×™×š â€¦", "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ"),

    ("×”×¢×•×‘×“ ××ª×—×™×™×‘ ×œ×©××•×¨ ×‘×¡×•×“ ×›×œ ××™×“×¢", "×—×•×–×”_×¢×‘×•×“×”"),

    ("×”×©×•×›×¨ ××ª×—×™×™×‘ ×œ×”×—×–×™×¨ ××ª ×”× ×›×¡ ×›×©×”×•× × ×§×™", "×—×•×–×”_×©×›×™×¨×•×ª"),

    ("×”×¦×“×“×™× ××¡×›×™××™× ×©×œ× ×œ×’×œ×•×ª ××™×“×¢ ×¡×•×“×™", "NDA"),

    ("×”× ×ª×‘×¢ ×‘×™×¦×¢ ×¨×©×œ× ×•×ª â€¦ ×œ×¤×™×›×š ××ª×‘×§×© ×‘×™×”××´×©", "×›×ª×‘_×ª×‘×™×¢×”"),

    ("×”××©×›×™×¨ ××©×›×™×¨ ×‘×–×” ×œ×©×•×›×¨ ×•×”×©×•×›×¨ ×©×•×›×¨ ×‘×–×” ××”××©×›×™×¨ ××ª ×”×“×™×¨×” ×”××¦×•×™×” ×‘×¨×—×•×‘", "×—×•×–×”_×©×›×™×¨×•×ª"),

    ("×ª×§×•×¤×ª ×”×©×›×™×¨×•×ª ×ª×—×œ ×‘×™×•× 1.1.2024 ×•×ª×¡×ª×™×™× ×‘×™×•× 31.12.2024. ×“××™ ×”×©×›×™×¨×•×ª ×™×¢××“×• ×¢×œ ×¡×š", "×—×•×–×”_×©×›×™×¨×•×ª"),

    ("×”×¢×•×‘×“ ××ª×—×™×™×‘ ×œ×‘×¦×¢ ××ª ×ª×¤×§×™×“×• ×‘××¡×™×¨×•×ª ×•×‘×”×ª×× ×œ×”×•×¨××•×ª ×”××¢×¡×™×§", "×—×•×–×”_×¢×‘×•×“×”"),

    ("×”×—×‘×¨×” ×ª×¡×¤×§ ×©×™×¨×•×ª×™ ×™×™×¢×•×¥ ×•×ª××™×›×” ×˜×›× ×™×ª ×œ×œ×§×•×— ×‘××©×š 12 ×—×•×“×©×™×", "×—×•×–×”_×©×™×¨×•×ª"),

    ("×× ×• ×“×•×¨×©×™× ××ª ×ª×©×œ×•× ×”×—×•×‘ ×‘×¡×š 10,000 ×©\"×— ×ª×•×š 7 ×™××™× ×××•×¢×“ ××›×ª×‘ ×–×”.", "××›×ª×‘_×”×ª×¨××”"),

    ("×”×—×œ×˜×ª ×‘×™×ª ×”××©×¤×˜ ×§×•×‘×¢×ª ×›×™ ×¢×œ ×”× ×ª×‘×¢ ×œ×©×œ× ×¤×™×¦×•×™×™× ×œ×ª×•×‘×¢", "×¤×¡×§_×“×™×Ÿ"),

    ("×ª×§× ×•×Ÿ ×–×” ××’×“×™×¨ ××ª ×›×œ×œ×™ ×”×”×ª× ×”×œ×•×ª ×•×”×–×›×•×™×•×ª ×©×œ ×—×‘×¨×™ ×”×¢××•×ª×”", "×ª×§× ×•×Ÿ"),

    ("×”×¡×›××ª ×”×¦×“×“×™× ×œ×‘×™×¦×•×¢ ×”×¢×‘×•×“×” ×¢×œ ×™×“×™ ×”×§×‘×œ×Ÿ ×‘×”×ª×× ×œ××¤×¨×˜ ×˜×›× ×™", "×—×•×–×”_×©×™×¨×•×ª"),

    ("×œ×¤×™ ×”×—×œ×˜×ª ×‘×™×ª ×”×“×™×Ÿ ×”××–×•×¨×™ ×œ×¢×‘×•×“×” ×‘×—×™×¤×” ××™×•× 1.1.2023", "×¤×¡×§_×“×™×Ÿ")

]



CLS_SYS = (

    "××ª×” ××¡×•×•×’ ××¡××›×™× ××©×¤×˜×™×™× ×‘×¢×‘×¨×™×ª ×‘××•×¤×Ÿ ××“×•×™×§ ×•×××™×Ÿ. "

    "×”×—×–×¨ JSON ×‘××‘× ×”:\n"

    '{"label":"<LABEL>","confidence":0-100}\n'

    f"×¢×œ×™×š ×œ×‘×—×•×¨ *×¨×§* ××ª×•×š ×”×ª×•×•×™×•×ª ×”×‘××•×ª: {', '.join(DOC_LABELS.keys())}. "

    "×—×©×•×‘ ×××•×“: ×× ××™× ×š ×‘×˜×•×— ×‘×¡×™×•×•×’ ×”××¡××š, ×‘×—×¨ '××¡××š_××—×¨'. "

    "×”×™×× ×¢ ×× ×™×—×•×©×™×. ×”×”×—×œ×˜×” ×—×™×™×‘×ª ×œ×”×™×•×ª ××‘×•×¡×¡×ª ×¢×œ ×ª×•×›×Ÿ ×”××¡××š ×‘×œ×‘×“."

)



def classify_doc(txt: str) -> str:

    if not isinstance(txt, str):

        try:

            txt = txt.decode("utf-8")

        except Exception:

            txt = str(txt)



    chunks_for_cls = []

    text_len = len(txt)

    if text_len > 1500:

        chunks_for_cls.append(txt[:500])

        chunks_for_cls.append(txt[text_len//4 - 250 : text_len//4 + 250])

        chunks_for_cls.append(txt[text_len//2 - 250 : text_len//2 + 250])

        chunks_for_cls.append(txt[text_len*3//4 - 250 : text_len*3//4 + 250])

        chunks_for_cls.append(txt[-500:])

    elif text_len > 500:

        chunks_for_cls.append(txt[:500])

        chunks_for_cls.append(txt[-500:])

    else:

        chunks_for_cls.append(txt)



    msgs = [{"role": "system", "content": CLS_SYS}]



    for eg, lbl in CLS_EXAMPLES:

        msgs += [

            {"role": "user", "content": eg},

            {"role": "assistant", "content": json.dumps({"label": lbl, "confidence": 95})}

        ]



    for c in chunks_for_cls:

        msgs.append({"role": "user", "content": c})



    try:

        r = client_sync_openai.chat.completions.create(

            model="gpt-4o-mini",

            messages=msgs,

            temperature=0,

            max_tokens=30

        )

        response_content = r.choices[0].message.content

        j = json.loads(response_content)

        if j.get("label") in DOC_LABELS:

            return j["label"]

        else:

            return "××¡××š_××—×¨"

    except json.JSONDecodeError:

        return "××¡××š_××—×¨"

    except Exception:

        return "××¡××š_××—×¨"



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

H = lambda s: s + "  **×¢× ×” ×‘×¢×‘×¨×™×ª ××œ××” ×•×œ×œ× ××™×œ×™× ×‘×× ×’×œ×™×ª. ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§ ××• ×¤×¡×´×“) ××—×¨×™ ×›×œ ×§×‘×™×¢×”.**"

PROMPTS = {

    "××›×ª×‘_×¤×™×˜×•×¨×™×Ÿ": dict(

        summary=H("×¡×›× ××›×ª×‘ ×¤×™×˜×•×¨×™×Ÿ: 1. ×¤×¨×˜×™ ×¢×•×‘×“ ×•×ª××¨×™×›×™×, 2. ×–×›×•×™×•×ª ×•×ª×©×œ×•××™×, 3. ×¦×¢×“×™× ××•××œ×¦×™×."),

        answer=H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. ×”×©×‘ ×¨×§ ×¢×œ ×¡××š ×”××›×ª×‘ ×•×—×•×§×™ ×¢×‘×•×“×” ×¨×œ×•×•× ×˜×™×™×.")

    ),

    "×—×•×–×”_×¢×‘×•×“×”": dict(

        summary=H("×¡×›× ×—×•×–×” ×¢×‘×•×“×”: 1. ×ª× ××™ ×”×¢×¡×§×”, 2. ×¡×¢×™×¤×™ ×¡×•×“×™×•×ª ×•××™-×ª×—×¨×•×ª, 3. ×¡×™×›×•× ×™× ×•×”××œ×¦×•×ª."),

        answer=H("××ª×” ×¢×•\"×“ ×“×™× ×™-×¢×‘×•×“×”. × ×ª×— ××ª ×¡×¢×™×¤×™ ×”×—×•×–×”.")

    ),

    "×—×•×–×”_×©×›×™×¨×•×ª": dict(

        summary=H(

            "×¡×›× ××ª ×—×•×–×” ×”×©×›×™×¨×•×ª ×œ× ×§×•×“×•×ª ×§×¨×™×˜×™×•×ª: "

            "1. ××™×”× ×”×¦×“×“×™× (××©×›×™×¨, ×©×•×›×¨, ×¢×¨×‘×™×) ×•×ª××¨×™×›×™× ×¢×™×§×¨×™×™× (×ª×§×•×¤×ª ×©×›×™×¨×•×ª, ××•×¤×¦×™×•×ª ×”××¨×›×”). "

            "2. ××”× ×“××™ ×”×©×›×™×¨×•×ª, ××•×¢×“×™ ×”×ª×©×œ×•× ×•×“×¨×›×™ ×”×¦××“×”. "

            "3. ××”×Ÿ ×—×•×‘×•×ª ×•×–×›×•×™×•×ª ×¢×™×§×¨×™×•×ª ×©×œ ×”×©×•×›×¨ (×œ×“×•×’××”, ××—×–×§×”, ×ª×™×§×•× ×™× ×§×˜× ×™×, ×ª×©×œ×•× ×—×©×‘×•× ×•×ª, ×©×™××•×© ×‘× ×›×¡) "

            "   ×•×©×œ ×”××©×›×™×¨ (×œ×“×•×’××”, ×ª×™×§×•× ×™× ×’×“×•×œ×™×, ××ª×Ÿ ×©×™×¨×•×ª×™×). "

            "4. ××”× ×”×‘×™×˜×—×•× ×•×ª ×•×”×¢×¨×‘×•×™×•×ª ×”× ×“×¨×©×™× (×©×™×§ ×‘×™×˜×—×•×Ÿ, ×©×˜×¨ ×—×•×‘, ×¢×¨×‘×•×ª ×‘× ×§××™×ª/×¦×“ ×’'). "

            "5. ××”× ×ª× ××™ ×¡×™×•× ×”×”×¡×›× ××• ×”×¤×¨×ª×• (×œ×“×•×’××”, ××¤×©×¨×•×ª ×™×¦×™××” ××•×§×“××ª, ×¡×¢×™×¤×™ ×¤×™×¦×•×™ ××•×¡×›×, ×¡×¢×™×¤×™ ×”×¤×¨×” ×™×¡×•×“×™×ª)."

            "\n\n×›×ª×•×‘ 4-6 Bullet-×™× ×§×¦×¨×™× (×¢×“ 40 ××™×œ×™× ×›×œ ××—×“):"

        ),

        answer=H("××ª×” ×¢×•\"×“ ×”××ª××—×” ×‘×“×™× ×™ ×—×•×–×™× ×•××§×¨×§×¢×™×Ÿ. × ×ª×— ××ª ×¡×¢×™×¤×™ ×”×—×•×–×” ×•××©××¢×•×ª× ×”××©×¤×˜×™×ª ×‘×”×ª×× ×œ×©××œ×ª ×”××©×ª××©.")

    ),

    "×—×•×–×”_×©×™×¨×•×ª": dict(

        summary=H("×¡×›× ×—×•×–×” ×©×™×¨×•×ª: 1. ××”×•×ª ×”×©×™×¨×•×ª, 2. ×ª× ××™ ×ª×©×œ×•×, 3. ×”×ª×—×™×™×‘×•×™×•×ª ×”×¦×“×“×™×, 4. ×ª× ××™ ×¡×™×•×."),

        answer=H("××ª×” ×¢×•\"×“ ××¡×—×¨×™. × ×ª×— ××ª ×ª× ××™ ×”×”×ª×§×©×¨×•×ª.")

    ),

    "××›×ª×‘_×”×ª×¨××”": dict(

        summary=H("×¡×›× ××›×ª×‘ ×”×ª×¨××”: 1. ×–×”×•×ª ×”×©×•×œ×— ×•×”× ××¢×Ÿ, 2. ××”×•×ª ×”×“×¨×™×©×”/×˜×¢× ×”, 3. ×”×¡×¢×“ ×”× ×“×¨×© ×•××•×¢×“ ×œ×ª×’×•×‘×”, 4. ×”×©×œ×›×•×ª ××™-×ª×’×•×‘×”."),

        answer=H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”. ×”×¡×‘×¨ ××ª ×”××©××¢×•×ª ×”××©×¤×˜×™×ª ×©×œ ×”××›×ª×‘ ×•××ª ×”×¦×¢×“×™× ×”× ×“×¨×©×™×.")

    ),

    "×ª×§× ×•×Ÿ": dict(

        summary=H("×¡×›× ×ª×§× ×•×Ÿ/××“×™× ×™×•×ª: 1. ××˜×¨×•×ª, 2. ×–×›×•×™×•×ª/×—×•×‘×•×ª, 3. ×¡×™×›×•× ×™× ×œ××™-×¦×™×•×ª."),

        answer=H("××ª×” ×¢×•\"×“ ×—×‘×¨×•×ª. ×”×¡×‘×¨ ×ª×•×§×£ ×¡×¢×™×¤×™ ×”×ª×§× ×•×Ÿ.")

    ),

    "NDA": dict(

        summary=H("×¡×›× NDA: 1. ×”×¦×“×“×™×, 2. ××”×• ××™×“×¢ ×¡×•×“×™, 3. ×”×ª×—×™×™×‘×•×™×•×ª ×¡×•×“×™×•×ª, 4. ×—×¨×™×’×™× ×•×ª×•×§×£."),

        answer=H("××ª×” ×¢×•\"×“ ××¡×—×¨×™. × ×ª×— ××ª ×¡×¢×™×¤×™ ×”×¡×•×“×™×•×ª ×•×”×©×œ×›×•×ª×™×”×.")

    ),

    "×›×ª×‘_×ª×‘×™×¢×”": dict(

        summary=H("×¡×›× ×›×ª×‘ ×ª×‘×™×¢×”: 1. ×¦×“×“×™×, 2. ×¢×™×œ×•×ª ×”×ª×‘×™×¢×”, 3. ×”×¡×¢×“×™× ×”× ×“×¨×©×™×, 4. ×œ×•×— ×–×× ×™× ×“×™×•× ×™."),

        answer=H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”. × ×ª×— ××ª ×¢×™×œ×•×ª ×”×ª×‘×™×¢×” ×•×”×¦×¢×“×™× ×”××¤×©×¨×™×™× ×œ×”×’× ×”.")

    ),

    "×›×ª×‘_×”×’× ×”": dict(

        summary=H("×¡×›× ×›×ª×‘ ×”×’× ×”: 1. ×¦×“×“×™×, 2. ×˜×¢× ×•×ª ×”×”×’× ×” ×”××¨×›×–×™×•×ª, 3. ×¢×•×‘×“×•×ª × ×˜×¢× ×•×ª, 4. ×¡×¢×“×™× × ×“×¨×©×™×."),

        answer=H("××ª×” ×¢×•\"×“ ×œ×™×˜×™×’×¦×™×”. × ×ª×— ××ª ×˜×¢× ×•×ª ×”×”×’× ×” ×”××•×¢×œ×•×ª.")

    ),

    "×¤×¡×§_×“×™×Ÿ": dict(

        summary=H("×¡×›× ×¤×¡×§-×“×™×Ÿ: 1. ×©××œ×” ××©×¤×˜×™×ª ××¨×›×–×™×ª, 2. ×¢×•×‘×“×•×ª ×¨×œ×•×•× ×˜×™×•×ª, 3. ×”×›×¨×¢×•×ª ×•×§×‘×™×¢×•×ª ×‘×™×ª-×”××©×¤×˜, 4. ×”×œ×›×” ××©×¤×˜×™×ª ×©× ×§×‘×¢×”."),

        answer=H("××ª×” ×¢×•\"×“. ×”×¡×‘×¨ ××ª ×”×œ×›×ª ×‘×™×ª-×”××©×¤×˜ ×•×”×©×œ×›×•×ª×™×”.")

    ),

    "××¡××š_××—×¨": dict(

        summary=H("×¡×›× ××ª ×”××¡××š ×‘×§×¦×¨×”: ×ª×§×¦×™×¨ ×›×œ×œ×™, × ×§×•×“×•×ª ×¢×™×§×¨×™×•×ª, ×”×©×œ×›×•×ª ××¤×©×¨×™×•×ª."),

        answer=H("××ª×” ×¢×•\"×“. × ×ª×— ××ª ×”××¡××š ×‘××•×¤×Ÿ ×›×œ×œ×™.")

    )

}

tmpl = lambda l, k: PROMPTS.get(l, PROMPTS["××¡××š_××—×¨"])[k]



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ retrieval (RAG) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

embed = lambda t: model.encode([t], normalize_embeddings=True)[0] if model else np.array([0.0])



async def retrieve(q, doc):

    if not model or not judgment_index or not law_index:

        return [], []



    q_emb = embed(q)

    secs = [embed(c) for c in chunk_text(doc, 400)] if doc else []

    cand = {"law": {}, "judg": {}}



    async def add(match, kind):

        meta, score = match.get("metadata", {}), match.get("score", 0)

        key = "IsraelLawID" if kind == "law" else "CaseNumber"

        _id = meta.get(key)

        if not _id:

            return

        coll = law_coll if kind == "law" else judgment_coll

        try:

            d = coll.find_one({key: _id})

            if d:

                cand[kind].setdefault(_id, {"doc": d, "scores": []})["scores"].append(score)

        except Exception:

            pass



    async def query(idx, vec):

        if not idx: return []

        vec_list = vec.tolist() if isinstance(vec, np.ndarray) else vec

        try:

            return idx.query(

                vector=vec_list,

                top_k=5,

                include_metadata=True

            ).get("matches", [])

        except Exception:

            return []



    async def scan(vec):

        for m in await query(law_index, vec):

            await add(m, "law")

        for m in await query(judgment_index, vec):

            await add(m, "judg")



    await asyncio.gather(scan(q_emb), *(scan(e) for e in secs[:10]))



    top = lambda d: sorted(d.values(), key=lambda x: -np.mean(x["scores"]))[:3]

    return [x["doc"] for x in top(cand["law"])], [x["doc"] for x in top(cand["judg"])]



async def citations_ok(ans: str) -> bool:

    pat = r'\[\d+\]|\(\d+\)'

    lines = [l.strip() for l in ans.splitlines() if l.strip()]

    cited_lines = sum(1 for l in lines if re.search(pat, l))

    return len(lines) > 0 and (cited_lines / len(lines)) >= 0.5



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ gen â€“ ×ª×©×•×‘×” ××©×¤×˜×™×ª ××§×¦×•×¢×™×ª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def gen(q: str) -> str:

    laws, judg = await retrieve(q, st.session_state.get("doc", ""))



    SNIPPET = 400

    def fmt_sources(lst, tag):

        if not lst:

            return f"×œ× × ××¦××• {tag} ×¨×œ×•×•× ×˜×™×™×."

        return "\n".join(

            f"[{i}] {d.get('Name', '×œ×œ× ×©×')} â€“ {(d.get('Description', '') or '')[:SNIPPET].strip()}"

            for i, d in enumerate(lst, 1)

        )



    doc_for_llm = st.session_state.get('doc', '')[:4000]



    sys = (

        tmpl(st.session_state.get("doctype", "××¡××š_××—×¨"), "answer") +

        "\n\n×”× ×—×™×•×ª × ×™×¡×•×— (×—×•×‘×”):\n"

        "â€¢ ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ××œ××” ×‘×œ×‘×“ â€“ ××™×Ÿ ×œ×”×©×ª××© ×‘×× ×’×œ×™×ª.\n"

        "â€¢ ×”×©×ª××© ×‘×œ×©×•×Ÿ ××©×¤×˜×™×ª-××§×¦×•×¢×™×ª, ×¤×¡×§××•×ª/×¡×¢×™×¤×™× ×××•×¡×¤×¨×™× (×œ×“×•×’××”: 1.2.3). ×‘××™×“×ª ×”×¦×•×¨×š ×¦×•×¨ ×¨×©×™××•×ª ×‘× ×§×•×“×•×ª (×‘×•×œ×˜×™×).\n"

        "â€¢ ×”×¡×ª××š ××š ×•×¨×§ ×¢×œ ×”××§×•×¨×•×ª ×©×œ××˜×”, ×•×¦×™×™×Ÿ ×‘×¡×•×’×¨×™×™× ××¨×•×‘×¢×™× ××ª ××¡×¤×¨-×”××§×•×¨ ×œ×™×“ ×›×œ ×§×‘×™×¢×” ××• ×¤×™×¡×§×” ×¨×œ×•×•× ×˜×™×ª, ×œ×“×•×’××”: [1] ××• [2,3].\n"

        "â€¢ ××œ ×ª×¦×™×™×Ÿ ××™×“×¢ ×©×œ× × ××¦× ×‘××§×•×¨×•×ª. ×× ××™×“×¢ ×—×¡×¨, ×¦×™×™×Ÿ ×–××ª ×‘××¤×•×¨×©.\n"

        f"\n--- ××¡××š ×©×”×•×¢×œ×” (×—×œ×§ ×¨××©×•× ×™) ---\n{doc_for_llm}" +

        f"\n\n--- ×—×•×§×™× ×¨×œ×•×•× ×˜×™×™× ---\n{fmt_sources(laws, '×—×•×§×™×')}" +

        f"\n\n--- ×¤×¡×§×™ ×“×™×Ÿ ×¨×œ×•×•× ×˜×™×™× ---\n{fmt_sources(judg, '×¤×¡×§×™ ×“×™×Ÿ')}"

    )



    try:

        r = await client_async_openai.chat.completions.create(

            model="gpt-4o-mini",

            messages=[

                {"role": "system", "content": sys},

                {"role": "user", "content": q}

            ],

            temperature=0.2,

            max_tokens=1200

        )

        return r.choices[0].message.content.strip()

    except Exception:

        return "××™×¨×¢×” ×©×’×™××” ×‘×ª×©×•×‘×”. ×× × × ×¡×” ×©×•×‘ ×××•×—×¨ ×™×•×ª×¨."



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ chat assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chat_assistant():

    st.markdown('<div class="chat-header">ğŸ’¬ Ask Mini Lawyer</div>', unsafe_allow_html=True)



    if "cid" not in st.session_state:

        cid = ls_get("AMLChatId") or str(uuid.uuid4())

        ls_set("AMLChatId", cid)

        st.session_state.cid = cid



    if "messages" not in st.session_state:

        try:

            conv = conv_coll.find_one({"local_storage_id": st.session_state.cid})

            st.session_state["messages"] = conv.get("messages", []) if conv else []

        except Exception:

            st.session_state["messages"] = []



    if "user_name" not in st.session_state:

        stored = ls_get("AMLUserName")

        if stored:

            st.session_state["user_name"] = stored



    if "user_name" not in st.session_state:

        with st.form("name"):

            st.text_input("×”×›× ×¡ ×©× ×œ×”×ª×—×œ×ª ×©×™×—×”:", key="user_name_input")

            sub = st.form_submit_button("×”×ª×—×œ")

        if sub and st.session_state.get("user_name_input"):

            st.session_state["user_name"] = st.session_state["user_name_input"]

            ls_set("AMLUserName", st.session_state["user_name"])

            add_msg("assistant", f"×©×œ×•× {st.session_state['user_name']}, ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?")

            try:

                conv_coll.update_one(

                    {"local_storage_id": st.session_state.cid},

                    {"$set": {

                        "user_name": st.session_state["user_name"],

                        "messages": st.session_state["messages"]

                    }},

                    upsert=True

                )

            except Exception:

                pass

            st.rerun()

        return



    st.markdown('<div class="chat-container">', unsafe_allow_html=True)

    show_msgs()

    st.markdown("</div>", unsafe_allow_html=True)



    up = st.file_uploader("ğŸ“„ ×”×¢×œ×” ××¡××š (PDF ××• DOCX)", type=["pdf", "docx"])

    if up:

        with st.spinner("××¢×‘×“ ××¡××š ×•××¡×•×•×’..."):

            try:

                raw = read_pdf(up) if up.type == "application/pdf/test" else read_docx(up)

                st.session_state.doc = "\n".join(l for l in raw.splitlines() if heb.search(l))

                if not st.session_state.doc.strip():

                    st.warning("×”××¡××š ×¨×™×§ ××• ×œ× ××›×™×œ ×ª×•×›×Ÿ ×¢×‘×¨×™ ×¨×œ×•×•× ×˜×™.")

                    if "doc" in st.session_state: del st.session_state.doc

                    if "doctype" in st.session_state: del st.session_state.doctype

                    if "summary" in st.session_state: del st.session_state.summary

                else:

                    st.session_state.doctype = classify_doc(st.session_state.doc)

                    st.success(f"×¡×•×’ ×”××¡××š ×©×¡×•×•×’: **{DOC_LABELS.get(st.session_state.doctype, st.session_state.doctype)}**")

                    if "summary" in st.session_state:

                        del st.session_state.summary

            except Exception:

                st.error("×©×’×™××” ×‘×¢×™×‘×•×“ ××• ×¡×™×•×•×’ ×”××¡××š.")

                if "doc" in st.session_state: del st.session_state.doc

                if "doctype" in st.session_state: del st.session_state.doctype

                if "summary" in st.session_state: del st.session_state.summary



    if hasattr(st.session_state, "doc") and st.session_state.doc.strip():

        if st.button("ğŸ“‹ ×¡×›× ××ª ×”××¡××š"):

            with st.spinner("××¡×›× ××ª ×”××¡××š..."):

                doc_for_summary = st.session_state.doc[:5000]

                prompt = tmpl(st.session_state.doctype, "summary") + "\n" + doc_for_summary

                try:

                    r = asyncio.run(

                        client_async_openai.chat.completions.create(

                            model="gpt-4o-mini",

                            messages=[{"role": "user", "content": prompt}],

                            temperature=0.1,

                            max_tokens=450

                        )

                    )

                    st.session_state.summary = ensure_he(

                        r.choices[0].message.content.strip().replace("â€¢", "â€“")

                    )

                except Exception:

                    st.error("×©×’×™××” ×‘×¡×™×›×•× ×”××¡××š.")

                    st.session_state.summary = "×œ× × ×™×ª×Ÿ ×œ×¡×›× ××ª ×”××¡××š."



    if st.session_state.get("summary"):

        st.markdown("### ×¡×™×›×•× ×”××¡××š:")

        st.markdown(

            f"<div dir='rtl' style='text-align:right'>{st.session_state.summary}</div>",

            unsafe_allow_html=True

        )

        st.markdown("---")



    async def handle(q):

        ans = ensure_he(await gen(q))

        if not await citations_ok(ans):

            ans2 = ensure_he(

                await gen(q + "\n×—×•×‘×” ×œ×¦×™×™×Ÿ ××§×•×¨ ×××•×¡×¤×¨ (×—×•×§/×¤×¡\"×“) ××—×¨×™ ×›×œ ××©×¤×˜ ××• ×¤×™×¡×§×” ×¨×œ×•×•× ×˜×™×ª.")

            )

            if await citations_ok(ans2):

                return ans2

            else:

                return ans

        return ans



    with st.form("ask", clear_on_submit=True):

        q = st.text_area("×”×§×œ×“ ×©××œ×” ××©×¤×˜×™×ª:", height=100, key="user_question_input")

        send = st.form_submit_button("×©×œ×—")



    if send and q.strip():

        add_msg("user", q.strip())

        with st.spinner("×× ×¡×— ×ª×©×•×‘×” ××©×¤×˜×™×ª..."):

            ans = asyncio.run(handle(q.strip()))

        add_msg("assistant", ans)

        try:

            conv_coll.update_one(

                {"local_storage_id": st.session_state.cid},

                {"$set": {

                    "messages": st.session_state["messages"],

                    "user_name": st.session_state["user_name"]

                }},

                upsert=True

            )

        except Exception:

            pass

        st.rerun()



    if st.button("ğŸ—‘ × ×§×” ×©×™×—×”"):

        try:

            conv_coll.delete_one({"local_storage_id": st.session_state.cid})

        except Exception:

            pass

        st_js_blocking("""

            localStorage.removeItem('AMLUserName');

            localStorage.removeItem('AMLChatId');

        """)

        st.session_state.clear()

        st.rerun()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ legal finder assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_document_details(kind, doc_id):

    coll = judgment_coll if kind == "Judgment" else law_coll

    key = "CaseNumber" if kind == "Judgment" else "IsraelLawID"

    try:

        return coll.find_one({key: doc_id})

    except Exception:

        return None



def get_explanation(scenario, doc, kind):

    name, desc = doc.get("Name", ""), doc.get("Description", "")

    if kind == "Judgment":

        prom = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:

{scenario}



×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×¤×¡×§ ×”×“×™×Ÿ ×”×‘×:

×©×: {name}

×ª×™××•×¨: {desc}



×”×¡×‘×¨ ×‘×§×¦×¨×” (×¢×“ 100 ××™×œ×™×) ××“×•×¢ ×¤×¡×§ ×“×™×Ÿ ×–×” ×¨×œ×•×•× ×˜×™ ×•×™×›×•×œ ×œ×¡×™×™×¢ ×œ×¡×¦× ×¨×™×•, ×•×“×¨×’ ××ª ××™×“×ª ×”×¨×œ×•×•× ×˜×™×•×ª ×‘×¦×™×•×Ÿ ×-0 ×¢×“ 10.

×”×—×–×¨ JSON ×‘××‘× ×”:

{{"advice":"×”×¡×‘×¨ ×§×¦×¨ ×¢×œ ×”×¨×œ×•×•× ×˜×™×•×ª","score":7}}"""

    else:

        prom = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×¡×¦× ×¨×™×• ×”×‘×:

{scenario}



×•×›×Ÿ ×¢×œ ×¤×¨×˜×™ ×”×—×•×§ ×”×‘×:

×©×: {name}

×ª×™××•×¨: {desc}



×”×¡×‘×¨ ×‘×§×¦×¨×” (×¢×“ 100 ××™×œ×™×) ××“×•×¢ ×—×•×§ ×–×” ×¨×œ×•×•× ×˜×™ ×•×™×›×•×œ ×œ×¡×™×™×¢ ×œ×¡×¦× ×¨×™×•, ×•×“×¨×’ ××ª ××™×“×ª ×”×¨×œ×•×•× ×˜×™×•×ª ×‘×¦×™×•×Ÿ ×-0 ×¢×“ 10.

×”×—×–×¨ JSON ×‘××‘× ×”:

{{"advice":"×”×¡×‘×¨ ×§×¦×¨ ×¢×œ ×”×¨×œ×•×•× ×˜×™×•×ª","score":6}}"""

    try:

        r = client_sync_openai.chat.completions.create(

            model="gpt-3.5-turbo",

            messages=[{"role": "user", "content": prom}],

            temperature=0.7,

            max_tokens=200

        )

        response_content = r.choices[0].message.content.strip()

        return json.loads(response_content)

    except json.JSONDecodeError:

        return {"advice": "×©×’×™××” ×‘×¤×¨×©× ×•×ª ×”×ª×©×•×‘×” ××”-AI.", "score": "N/A"}

    except Exception:

        return {"advice": "×©×’×™××” ×‘××—×–×•×¨ ×”×¡×‘×¨.", "score": "N/A"}



def legal_finder_assistant():

    st.title("âš–ï¸ Legal Finder Assistant")

    kind = st.selectbox("×‘×—×¨ ×¡×•×’ ××¡××š ×œ×—×™×¤×•×©", ["Judgment", "Law"])

    scen = st.text_area("×ª××¨ ××ª ×”×ª×¨×—×™×© ×”××©×¤×˜×™ ×©×œ×š (×¢×“ 500 ××™×œ×™×) ×œ×¦×•×¨×š ×—×™×¤×•×© ×¨×œ×•×•× ×˜×™:")

    

    if st.button("ğŸ” ××¦× ×ª×•×¦××•×ª ×¨×œ×•×•× ×˜×™×•×ª") and scen:

        if not model or (kind == "Judgment" and not judgment_index) or (kind == "Law" and not law_index):

            st.error("×¨×›×™×‘×™ ×—×™×¤×•×© ×œ× ×–××™× ×™×. ×× × ×•×•×“× ×©×”××•×“×œ ×•××™× ×“×§×¡×™ ×”-Pinecone ××•×’×“×¨×™×.")

            return



        with st.spinner("××—×¤×© ××¡××›×™× ×¨×œ×•×•× ×˜×™×™×..."):

            try:

                q_emb = model.encode([scen], normalize_embeddings=True)[0]

                idx = judgment_index if kind == "Judgment" else law_index

                key = "CaseNumber" if kind == "Judgment" else "IsraelLawID"

                

                matches = idx.query(

                    vector=q_emb.tolist(),

                    top_k=5,

                    include_metadata=True

                ).get("matches", [])



                if not matches:

                    st.info("×œ× × ××¦××• ×ª×•×¦××•×ª ×¨×œ×•×•× ×˜×™×•×ª ×¢×‘×•×¨ ×”×ª×¨×—×™×© ×©×ª×•××¨.")

                    return



                for m in matches:

                    _id = m.get("metadata", {}).get(key)

                    if not _id: continue



                    doc = load_document_details(kind, _id)

                    if not doc:

                        continue



                    name = doc.get("Name", f"×œ×œ× ×©× (ID: {_id})")

                    desc = doc.get("Description", "")

                    date_lbl = "DecisionDate" if kind == "Judgment" else "PublicationDate"

                    extra = (

                        f"<div class='law-meta'>×¡×•×’ ×”×œ×™×š: {doc.get('ProcedureType','N/A')}</div>"

                        if kind == "Judgment" else ""

                    )

                    st.markdown(

                        f"<div class='law-card'><div class='law-title'>{name} (ID:{_id})</div>"

                        f"<div class='law-description'>{desc}</div>"

                        f"<div class='law-meta'>{date_lbl}: {doc.get(date_lbl,'N/A')}</div>{extra}</div>",

                        unsafe_allow_html=True

                    )

                    with st.spinner("×× ×ª×— ×¨×œ×•×•× ×˜×™×•×ª ×¢× AI..."):

                        res = get_explanation(scen, doc, kind)

                    st.markdown(

                        f"<span style='color:red;'>**×¢×¦×ª ×”××¢×¨×›×ª (×¨×œ×•×•× ×˜×™×•×ª {res.get('score','N/A')}/10):** {res.get('advice','')}</span>",

                        unsafe_allow_html=True

                    )

                    with st.expander(f"×”×¦×’ ×¤×¨×˜×™× ××œ××™× ×¢×‘×•×¨ {name}"):

                        st.json(doc)

            except Exception:

                st.error("××™×¨×¢×” ×©×’×™××” ×‘×—×™×¤×•×©. ×× × ×•×•×“× ×©×›×œ ×”××¢×¨×›×•×ª ×¤×•×¢×œ×•×ª ×›×”×œ×›×”.")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if app_mode == "Chat Assistant":

    chat_assistant()

else:

    legal_finder_assistant()
