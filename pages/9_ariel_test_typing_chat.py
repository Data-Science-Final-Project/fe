
#  imports 
import os, re, json, uuid, asyncio
from datetime import datetime

import streamlit as st
import torch, fitz, docx, numpy as np
from bidi.algorithm import get_display
from dotenv import load_dotenv
from openai import AsyncOpenAI, OpenAI
from streamlit_js import st_js, st_js_blocking

from app_resources import mongo_client, pinecone_client, model

#  env / clients 
load_dotenv()
OPENAI_API_KEY = os.getenv("OPEN_AI")
DATABASE_NAME  = os.getenv("DATABASE_NAME")

client_async_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)
client_sync_openai  = OpenAI(api_key=OPENAI_API_KEY)

judgment_index = pinecone_client.Index("judgments-names")
law_index      = pinecone_client.Index("laws-names")

db = mongo_client[DATABASE_NAME]
judgment_coll = db["judgments"]
law_coll      = db["laws"]
conv_coll     = db["conversations"]

torch.classes.__path__ = []
os.environ["TOKENIZERS_PARALLELISM"] = "false"

#  UI CSS 
st.set_page_config("Ask Mini Lawyer", "锔", layout="wide")
st.markdown("""
<style>
.chat-container{background:#1E1E1E;padding:20px;border-radius:10px}
.chat-header{color:#4CAF50;font-size:36px;font-weight:bold;text-align:center}
.user-message{background:#4CAF50;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.bot-message{background:#44475a;color:#ecf2f8;padding:10px;border-radius:10px;margin:10px}
.timestamp{font-size:0.75em;color:#bbb}
.law-card{border:1px solid #e0e0e0;border-radius:10px;padding:20px;margin-bottom:15px;
          box-shadow:0 2px 4px rgba(0,0,0,0.1);background:#f9f9f9}
.law-title{font-size:20px;font-weight:bold;color:#333}
.law-description{font-size:16px;color:#444;margin:10px 0}
.law-meta{font-size:14px;color:#555}
.stButton>button{background:#7ce38b;color:#fff;font-size:14px;border:none;padding:8px 16px;border-radius:5px;cursor:pointer}
.stButton>button:hover{background:#69d67a}
</style>
""", unsafe_allow_html=True)

app_mode = st.sidebar.selectbox("Choose module", ["Chat Assistant", "Legal Finder"])

#  helpers 
def ls_get(key: str):
    
    with st.container():
        st.markdown("<div style='display:none'>", unsafe_allow_html=True)
        val = st_js_blocking(
            f"return localStorage.getItem('{key}');",
            key="ls_"+key
        )
        st.markdown("</div>", unsafe_allow_html=True)
    return val

def ls_set(key: str, val: str):
    st_js(f"localStorage.setItem('{key}', '{val}');")


heb = re.compile(r'[-转]')
SALUT = r'\b(|专|专\.?|\'?|专转|"专|"专\.?)\b'
def rtl_norm(t:str)->str:
    if not heb.search(t): return t
    try: t = get_display(t)
    except: t = " ".join(w[::-1] if heb.search(w) else w for w in t.split())
    return re.sub(r'\s{2,}', ' ', re.sub(SALUT, '', t)).strip()

read_pdf  = lambda f: "".join(rtl_norm(p.get_text()) for p in fitz.open(stream=f.read(), filetype="pdf"))
read_docx = lambda f: "\n".join(rtl_norm(p.text) for p in docx.Document(f).paragraphs if p.text.strip())

def add_msg(role, txt):
    st.session_state.setdefault("messages", []).append(
        {"role":role, "content":txt, "timestamp":datetime.now().strftime("%H:%M:%S")})

def show_msgs():
    for m in st.session_state.get("messages", []):
        css="user-message" if m["role"]=="user" else "bot-message"
        st.markdown(f"<div class='{css}'>{m['content']}<div class='timestamp'>{m['timestamp']}</div></div>", unsafe_allow_html=True)

def chunk_text(t,L=450):
    sent=re.split(r'(?:\.|\?|!)\s+',t); out,cur=[], ""
    for s in sent:
        if len(cur)+len(s)>L and cur: out.append(cur.strip()); cur=s
        else: cur+=" "+s
    if cur.strip(): out.append(cur.strip())
    return out[:20]

contains_en=lambda t: bool(re.search(r'[A-Za-z]',t))
def ensure_he(t):
    if sum(contains_en(w) for w in t.split())<=3: return t
    r=client_sync_openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role":"user","content":"转专 注专转 :\n"+t}],
        temperature=0,max_tokens=len(t)//2)
    return r.choices[0].message.content.strip()

#  classification 
DOC_LABELS = {
    "_注":      "住  注住拽 注  注 注",
    "_砖专转":     "住 砖专转 专, 砖专  住 专",
    "_砖专转":      "住   住驻拽 砖专转",
    "转_驻专":    "注 注 住 注住拽  驻住拽转 注",
    "转_转专":      "转 专砖  专 驻 拽转 ",
    "转拽":           "住  转 转 (: 转拽 专, 转专, 注转)",
    "NDA":             "住 住转 -",
    "转_转注":       "住 驻转转  转-砖驻",
    "转_":        "转 转 转注",
    "驻住拽_":         "专注转 转-砖驻",
    "住_专":        " 住 砖驻 专 砖 住 祝 拽专"
}

CLS_SYS = (
    "转 住 住 砖驻 注专转. "
    "拽专 转 拽住 爪专祝 专 *专拽* 转 砖 转转 转 :\n"
    + ", ".join(DOC_LABELS.keys()) +
    "\n 转 砖 拽住 住祝."
)

def classify_doc(txt: str) -> str:
    sample = txt[:1500]   # 拽注 爪, 拽 注转
    try:
        resp = client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": CLS_SYS},
                {"role": "user",   "content": sample}
            ],
            temperature=0,
            max_tokens=5
        )
        label = resp.choices[0].message.content.strip()
        return label if label in DOC_LABELS else "住_专"
    except Exception:
        return "住_专"



#  prompts 
H=lambda s: s+"  **注 注专转    转. 爪 拽专 住驻专 (拽  驻住状) 专  拽注.**"
PROMPTS={
 "转_驻专":dict(summary=H("住 转 驻专: 1. 驻专 注 转专, 2. 转 转砖, 3. 爪注 爪."),
                      answer =H("转 注\" -注. 砖 专拽 注 住 转 拽 注 专.")),
 "_注":  dict(summary=H("住  注: 1. 转 注住拽, 2. 住注驻 住转 -转专转, 3. 住 爪转."),
                      answer =H("转 注\" -注. 转 转 住注驻 .")),
 "转拽":        dict(summary=H("住 转拽/转: 1. 专转, 2. 转/转, 3. 住 -爪转."),
                      answer =H("转 注\" 专转. 住专 转拽祝 住注驻 转拽.")),
 "转_转注":    dict(summary=H("住 转 转注: 1. 注转, 2. 住注, 3.   ."),
                      answer =H("转 注\" 爪. 转 转 注转 转注.")),
 "驻住拽_":      dict(summary=H("住 驻住拽-: 1. 砖 砖驻转, 2. 拽注转, 3. ."),
                      answer =H("转 注\". 住专 转 转 转-砖驻.")),
 "_":             dict(summary=H("住 转 住: 转拽爪专, 拽转 注拽专转, 砖转."),
                      answer =H("转 注\"."))
}
tmpl=lambda l,k: PROMPTS.get(l,PROMPTS["_"])[k]

#  retrieval (RAG) 
embed=lambda t: model.encode([t],normalize_embeddings=True)[0]

async def retrieve(q,doc):
    q_emb=embed(q); secs=[embed(c) for c in chunk_text(doc)] if doc else []
    cand={"law":{}, "judg":{}}
    async def add(m,kind):
        meta,score=m.get("metadata",{}), m.get("score",0)
        key="IsraelLawID" if kind=="law" else "CaseNumber"; _id=meta.get(key)
        if not _id: return
        coll=law_coll if kind=="law" else judgment_coll
        d=coll.find_one({key:_id}); 
        if d: cand[kind].setdefault(_id,{"doc":d,"scores":[]})["scores"].append(score)
    async def scan(e):
        rl,rj=await asyncio.gather(
            asyncio.to_thread(law_index.query,vector=e.tolist(),top_k=1,include_metadata=True),
            asyncio.to_thread(judgment_index.query,vector=e.tolist(),top_k=1,include_metadata=True))
        [await add(m,"law") for m in rl.get("matches",[])]
        [await add(m,"judg")for m in rj.get("matches",[])]
    await asyncio.gather(*(scan(e) for e in secs))
    for m in law_index.query(vector=q_emb.tolist(),top_k=3,include_metadata=True).get("matches",[]):  await add(m,"law")
    for m in judgment_index.query(vector=q_emb.tolist(),top_k=3,include_metadata=True).get("matches",[]): await add(m,"judg")
    top=lambda d: sorted(d.values(), key=lambda x:-np.mean(x["scores"]))[:3]
    return [x["doc"] for x in top(cand["law"])], [x["doc"] for x in top(cand["judg"])]

async def citations_ok(ans:str)->bool:
    if contains_en(ans): return False
    try:
        r=await client_async_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"user","content":"Does every claim have an explicit citation? Answer Yes/No.\n"+ans}],
            temperature=0,max_tokens=3)
        return "yes" in r.choices[0].message.content.lower()
    except: return True

#  chat assistant 
def chat_assistant():
    st.markdown('<div class="chat-header"> Ask Mini Lawyer</div>', unsafe_allow_html=True)
    # cid & messages
    if "cid" not in st.session_state:
        cid=ls_get("AMLChatId") or str(uuid.uuid4()); ls_set("AMLChatId",cid); st.session_state.cid=cid
    if "messages" not in st.session_state:
        conv=conv_coll.find_one({"local_storage_id":st.session_state.cid})
        st.session_state["messages"]=conv.get("messages",[]) if conv else []

    # name persistence
    if "user_name" not in st.session_state:
        stored=ls_get("AMLUserName")
        if stored: st.session_state["user_name"]=stored
    if "user_name" not in st.session_state:
        with st.form("name"): st.text_input("住 砖 转转 砖:", key="user_name_input"); sub=st.form_submit_button("转")
        if sub and st.session_state.get("user_name_input"):
            st.session_state["user_name"]=st.session_state["user_name_input"]; ls_set("AMLUserName",st.session_state["user_name"])
            add_msg("assistant",f"砖 {st.session_state['user_name']},  驻砖专 注专?")
            conv_coll.update_one({"local_storage_id":st.session_state.cid},
                                 {"$set":{"user_name":st.session_state["user_name"],
                                          "messages":st.session_state["messages"]}}, upsert=True)
            st.rerun()
        return

    # history
    st.markdown('<div class="chat-container">',unsafe_allow_html=True); show_msgs(); st.markdown("</div>",unsafe_allow_html=True)

    # upload
    up=st.file_uploader(" 注 住",type=["pdf","docx"])
    if up:
        raw=read_pdf(up) if up.type=="application/pdf" else read_docx(up)
        st.session_state.doctype=classify_doc(raw)
        st.session_state.doc="\n".join(l for l in raw.splitlines() if heb.search(l))
        st.success(f"住 住: {st.session_state.doctype}")

    # summary
    if hasattr(st.session_state,"doc") and st.button(" 住"):
        with st.spinner("住..."):
            prompt=tmpl(st.session_state.doctype,"summary")+"\n"+st.session_state.doc
            r=asyncio.run(client_async_openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}],
                temperature=0,max_tokens=700))
            st.session_state.summary=ensure_he(r.choices[0].message.content.strip())
    if st.session_state.get("summary"):
        st.markdown("### 住:"); st.markdown(f"<div dir='rtl' style='text-align:right'>{st.session_state.summary}</div>",unsafe_allow_html=True)

    # answer helpers
    async def gen(q):
        laws,judg=await retrieve(q,st.session_state.get("doc",""))
        law_txt="\n\n".join(d.get("Description","")[:800] for d in laws) or " 爪 拽 专."
        jud_txt="\n\n".join(d.get("Description","")[:800] for d in judg) or " 爪 驻住拽  专."
        sys=tmpl(st.session_state.get("doctype","_"),"answer")+\
            f"\n\n--- 住 ---\n{st.session_state.get('doc','')[:1500]}" +\
            f"\n\n--- 拽 ---\n{law_txt}" +\
            f"\n\n--- 驻住拽  ---\n{jud_txt}"
        r=await client_async_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"system","content":sys},{"role":"user","content":q}],
            temperature=0,max_tokens=700)
        return r.choices[0].message.content.strip()

    async def handle(q):
        ans=ensure_he(await gen(q))
        if await citations_ok(ans): return ans
        ans2=ensure_he(await gen(q+"\n 爪 拽专 住驻专 (拽/驻住\") 专  砖驻."))
        return ans2

    with st.form("ask",clear_on_submit=True):
        q=st.text_area("拽 砖 砖驻转:",height=100); send=st.form_submit_button("砖")
    if send and q.strip():
        ans=asyncio.run(handle(q.strip())); add_msg("user",q.strip()); add_msg("assistant",ans)
        conv_coll.update_one({"local_storage_id":st.session_state.cid},
                             {"$set":{"messages":st.session_state["messages"],
                                      "user_name":st.session_state["user_name"]}}, upsert=True)
        st.rerun()

    if st.button(" 拽"):
        conv_coll.delete_one({"local_storage_id":st.session_state.cid})
        st_js("localStorage.clear()"); st.session_state.clear(); st.rerun()

#  legal finder assistant 
def load_document_details(kind, doc_id):
    coll = judgment_coll if kind=="Judgment" else law_coll
    key  = "CaseNumber" if kind=="Judgment" else "IsraelLawID"
    return coll.find_one({key:doc_id})

def get_explanation(scenario, doc, kind):
    name, desc = doc.get("Name",""), doc.get("Description","")
    if kind=="Judgment":
        prom=f"""转住住 注 住爪专 :
{scenario}

 注 驻专 驻住拽  :
砖: {name}
转专: {desc}

住专 拽爪专 注 驻住拽   住注 专 0-10.
专 JSON :
{{"advice":"住专","score":7}}"""
    else:
        prom=f"""转住住 注 住爪专 :
{scenario}

 注 驻专 拽 :
砖: {name}
转专: {desc}

住专 拽爪专 注 拽 专 专 0-10.
专 JSON :
{{"advice":"住专","score":6}}"""
    try:
        r=client_sync_openai.chat.completions.create(
            model="gpt-3.5-turbo",messages=[{"role":"user","content":prom}],temperature=0.7)
        return json.loads(r.choices[0].message.content.strip())
    except: return {"advice":"砖","score":"N/A"}

def legal_finder_assistant():
    st.title("Legal Finder Assistant")
    kind=st.selectbox("Choose what to search",["Judgment","Law"])
    scen=st.text_area("Describe your scenario")
    if st.button("Find Suitable Results") and scen:
        q_emb=model.encode([scen],normalize_embeddings=True)[0]
        idx=judgment_index if kind=="Judgment" else law_index
        key="CaseNumber" if kind=="Judgment" else "IsraelLawID"
        matches=idx.query(vector=q_emb.tolist(),top_k=5,include_metadata=True).get("matches",[])
        if not matches: st.info("No matches found."); return
        for m in matches:
            _id=m.get("metadata",{}).get(key); doc=load_document_details(kind,_id); 
            if not doc: continue
            name,desc=doc.get("Name",""),doc.get("Description","")
            date_lbl="DecisionDate" if kind=="Judgment" else "PublicationDate"
            extra=f"<div class='law-meta'>Procedure Type: {doc.get('ProcedureType','N/A')}</div>" if kind=="Judgment" else ""
            st.markdown(f"<div class='law-card'><div class='law-title'>{name} (ID:{_id})</div>"
                        f"<div class='law-description'>{desc}</div>"
                        f"<div class='law-meta'>{date_lbl}: {doc.get(date_lbl,'N/A')}</div>{extra}</div>",unsafe_allow_html=True)
            with st.spinner("GPT explanation..."): res=get_explanation(scen,doc,kind)
            st.markdown(f"<span style='color:red;'>注爪转 转专: {res.get('advice','')}</span>",unsafe_allow_html=True)
            with st.expander(f"View Full Details for {_id}"): st.json(doc)

#  main 
if app_mode=="Chat Assistant":
    chat_assistant()
else:
    legal_finder_assistant()
